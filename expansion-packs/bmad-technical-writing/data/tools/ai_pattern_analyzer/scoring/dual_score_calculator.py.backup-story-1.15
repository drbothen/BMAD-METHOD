"""
Dual score calculation module.

Calculates Detection Risk (0-100, lower=better) and Quality Score (0-100, higher=better)
with comprehensive breakdown across 22 dimensions and 174 quality points.

This is a complex scoring system extracted from the monolithic analyzer as part of
the modularization effort (Phase 3).

Research Sources:
- GPTZero methodology (perplexity & burstiness)
- Originality.AI pattern recognition
- Academic NLP studies on AI detection
- Stanford research on demographic bias
- MIT/Northeastern research on syntactic templates
"""

from datetime import datetime
from typing import List, Tuple

from ai_pattern_analyzer.core.results import AnalysisResults
from ai_pattern_analyzer.scoring.dual_score import (
    DualScore, ScoreCategory, ScoreDimension,
    ImprovementAction, THRESHOLDS
)


def calculate_dual_score(results: AnalysisResults,
                        detection_target: float = 30.0,
                        quality_target: float = 85.0) -> DualScore:
    """
    Calculate dual scores: Detection Risk (0-100, lower=better) and Quality Score (0-100, higher=better).

    This method analyzes 22 dimensions across 4 tiers:
    - Tier 1: Advanced Detection (70 points) - Highest accuracy metrics
    - Tier 2: Core Patterns (74 points) - Proven AI signatures
    - Tier 3: Supporting Indicators (46 points) - Context and quality
    - Tier 4: Advanced Structural Patterns (10 points) - AST-based patterns

    Args:
        results: AnalysisResults from analysis
        detection_target: Target detection risk (default 30 = low risk)
        quality_target: Target quality score (default 85 = excellent)

    Returns:
        DualScore with comprehensive breakdown and optimization path
    """
    timestamp = datetime.now().isoformat()

    # Map score levels to numerical values (0-1 scale)
    # Added empty string for selective loading compatibility
    score_map = {"VERY HIGH": 1.0, "HIGH": 1.0, "MEDIUM": 0.75, "LOW": 0.5, "VERY LOW": 0.25, "UNKNOWN": 0.5, "N/A": 0.5, "": 0.5}

    # ============================================================================
    # TIER 1: ADVANCED DETECTION (70 points) - Highest accuracy
    # ============================================================================

    # GLTR Token Ranking (12 points) - 95% accuracy on GPT-3/ChatGPT
    gltr_val = score_map.get(results.gltr_score, 0.5) if hasattr(results, 'gltr_score') and results.gltr_score else 0.5
    gltr_score = ScoreDimension(
        name="GLTR Token Ranking",
        score=gltr_val * 12,
        max_score=12.0,
        percentage=gltr_val * 100,
        impact=_calculate_impact(gltr_val, 12.0),
        gap=(1.0 - gltr_val) * 12,
        raw_value=getattr(results, 'gltr_top10_percentage', None),
        recommendation="Rewrite high-predictability segments (>70% top-10 tokens)" if gltr_val < 0.75 else None
    )

    # Advanced Lexical Diversity - HDD/Yule's K (8 points)
    lexical_val = score_map.get(getattr(results, 'advanced_lexical_score', 'UNKNOWN'), 0.5)
    lexical_score = ScoreDimension(
        name="Advanced Lexical (HDD/Yule's K)",
        score=lexical_val * 8,
        max_score=8.0,
        percentage=lexical_val * 100,
        impact=_calculate_impact(lexical_val, 8.0),
        gap=(1.0 - lexical_val) * 8,
        raw_value=getattr(results, 'hdd_score', None),
        recommendation="Increase vocabulary diversity (target HDD > 0.65)" if lexical_val < 0.75 else None
    )

    # Advanced Lexical: MATTR (Moving Average Type-Token Ratio) - 12 points
    mattr_assessment = getattr(results, 'mattr_assessment', None)
    mattr_score_val = 12.0 * (1.0 if mattr_assessment == 'EXCELLENT' else
                               0.75 if mattr_assessment == 'GOOD' else
                               0.42 if mattr_assessment == 'FAIR' else 0.0) if mattr_assessment else 0.0
    mattr_dim = ScoreDimension(
        name="MATTR (Lexical Richness)",
        score=mattr_score_val,
        max_score=12.0,
        percentage=(mattr_score_val / 12.0) * 100,
        impact=_calculate_impact(mattr_score_val / 12.0, 12.0),
        gap=12.0 - mattr_score_val,
        raw_value=getattr(results, 'mattr', None),
        recommendation="Increase vocabulary variety (target MATTR ≥0.70)" if mattr_score_val < 9.0 else None
    )

    # Advanced Lexical: RTTR (Root Type-Token Ratio) - 8 points
    rttr_assessment = getattr(results, 'rttr_assessment', None)
    rttr_score_val = 8.0 * (1.0 if rttr_assessment == 'EXCELLENT' else
                             0.75 if rttr_assessment == 'GOOD' else
                             0.375 if rttr_assessment == 'FAIR' else 0.0) if rttr_assessment else 0.0
    rttr_dim = ScoreDimension(
        name="RTTR (Global Diversity)",
        score=rttr_score_val,
        max_score=8.0,
        percentage=(rttr_score_val / 8.0) * 100,
        impact=_calculate_impact(rttr_score_val / 8.0, 8.0),
        gap=8.0 - rttr_score_val,
        raw_value=getattr(results, 'rttr', None),
        recommendation="Add domain-specific terminology (target RTTR ≥7.5)" if rttr_score_val < 6.0 else None
    )

    # AI Detection Ensemble - DetectGPT/RoBERTa (10 points)
    ai_detect_val = score_map.get(getattr(results, 'ai_detection_score', 'UNKNOWN'), 0.5)
    ai_detect_score = ScoreDimension(
        name="AI Detection Ensemble",
        score=ai_detect_val * 10,
        max_score=10.0,
        percentage=ai_detect_val * 100,
        impact=_calculate_impact(ai_detect_val, 10.0),
        gap=(1.0 - ai_detect_val) * 10,
        raw_value=getattr(results, 'roberta_sentiment_variance', None),
        recommendation="Increase emotional variation (sentiment variance > 0.15)" if ai_detect_val < 0.75 else None
    )

    # Stylometric Markers (10 points)
    stylo_val = score_map.get(getattr(results, 'stylometric_score', 'UNKNOWN'), 0.5)
    stylo_score = ScoreDimension(
        name="Stylometric Markers",
        score=stylo_val * 10,
        max_score=10.0,
        percentage=stylo_val * 100,
        impact=_calculate_impact(stylo_val, 10.0),
        gap=(1.0 - stylo_val) * 10,
        raw_value=getattr(results, 'however_per_1k', None),
        recommendation="Reduce AI transitions (however/moreover), passive voice <12%, vary function words" if stylo_val < 0.75 else None
    )

    # Syntactic Complexity (4 points)
    syntax_val = score_map.get(getattr(results, 'syntactic_score', 'UNKNOWN'), 0.5)
    syntax_score = ScoreDimension(
        name="Syntactic Complexity",
        score=syntax_val * 4,
        max_score=4.0,
        percentage=syntax_val * 100,
        impact=_calculate_impact(syntax_val, 4.0),
        gap=(1.0 - syntax_val) * 4,
        raw_value=getattr(results, 'subordination_index', None),
        recommendation="Add subordinate clauses, vary tree depth" if syntax_val < 0.75 else None
    )

    # Multi-Model Perplexity Consensus (6 points)
    multi_perp_score_val = 3.0  # Default
    avg_perp = None
    if hasattr(results, 'gpt2_perplexity') and hasattr(results, 'distilgpt2_perplexity'):
        if results.gpt2_perplexity and results.distilgpt2_perplexity:
            gpt2_human = results.gpt2_perplexity > 120
            distil_human = results.distilgpt2_perplexity > 120
            avg_perp = (results.gpt2_perplexity + results.distilgpt2_perplexity) / 2

            if gpt2_human and distil_human:
                multi_perp_score_val = 6.0
            elif not gpt2_human and not distil_human:
                multi_perp_score_val = 0.0
            else:
                multi_perp_score_val = 3.0

    multi_perp_dim = ScoreDimension(
        name="Multi-Model Perplexity Consensus",
        score=multi_perp_score_val,
        max_score=6.0,
        percentage=(multi_perp_score_val / 6.0) * 100,
        impact=_calculate_impact(multi_perp_score_val / 6.0, 6.0),
        gap=6.0 - multi_perp_score_val,
        raw_value=avg_perp,
        recommendation="Increase unpredictability: use diverse vocabulary, unexpected word choices" if multi_perp_score_val < 4.5 else None
    )

    advanced_category = ScoreCategory(
        name="Advanced Detection",
        total=gltr_score.score + lexical_score.score + mattr_dim.score + rttr_dim.score + ai_detect_score.score + stylo_score.score + syntax_score.score + multi_perp_dim.score,
        max_total=70.0,
        percentage=((gltr_score.score + lexical_score.score + mattr_dim.score + rttr_dim.score + ai_detect_score.score + stylo_score.score + syntax_score.score + multi_perp_dim.score) / 70.0) * 100,
        dimensions=[gltr_score, lexical_score, mattr_dim, rttr_dim, ai_detect_score, stylo_score, syntax_score, multi_perp_dim]
    )

    # ============================================================================
    # TIER 2: CORE PATTERNS (74 points) - Proven AI signatures
    # ============================================================================

    # Burstiness - Sentence Variation (12 points)
    burst_val = score_map[results.burstiness_score]
    burst_score = ScoreDimension(
        name="Burstiness (Sentence Variation)",
        score=burst_val * 12,
        max_score=12.0,
        percentage=burst_val * 100,
        impact=_calculate_impact(burst_val, 12.0),
        gap=(1.0 - burst_val) * 12,
        raw_value=results.sentence_stdev,
        recommendation="Vary sentence lengths: short (5-10w), medium (15-25w), long (30-45w)" if burst_val < 0.75 else None
    )

    # Perplexity - Vocabulary (10 points)
    perp_val = score_map[results.perplexity_score]
    perp_score = ScoreDimension(
        name="Perplexity (Vocabulary)",
        score=perp_val * 10,
        max_score=10.0,
        percentage=perp_val * 100,
        impact=_calculate_impact(perp_val, 10.0),
        gap=(1.0 - perp_val) * 10,
        raw_value=results.ai_vocabulary_per_1k,
        recommendation="Replace AI vocabulary: delve, leverage, robust, harness" if perp_val < 0.75 else None
    )

    # Formatting Patterns (8 points)
    format_val = score_map[results.formatting_score]
    format_score = ScoreDimension(
        name="Formatting Patterns",
        score=format_val * 8,
        max_score=8.0,
        percentage=format_val * 100,
        impact=_calculate_impact(format_val, 8.0),
        gap=(1.0 - format_val) * 8,
        raw_value=results.em_dashes_per_page,
        recommendation="Reduce em-dashes to ≤2 per page, reduce bold/italic density" if format_val < 0.75 else None
    )

    # Voice & Authenticity (10 points)
    voice_val = score_map[results.voice_score]
    voice_score = ScoreDimension(
        name="Voice & Authenticity",
        score=voice_val * 10,
        max_score=10.0,
        percentage=voice_val * 100,
        impact=_calculate_impact(voice_val, 10.0),
        gap=(1.0 - voice_val) * 10,
        raw_value=results.first_person_count,
        recommendation="Add personal perspective, contractions, direct address" if voice_val < 0.75 else None
    )

    # Structure & Organization (8 points)
    structure_val = score_map[results.structure_score]
    structure_score = ScoreDimension(
        name="Structure & Organization",
        score=structure_val * 8,
        max_score=8.0,
        percentage=structure_val * 100,
        impact=_calculate_impact(structure_val, 8.0),
        gap=(1.0 - structure_val) * 8,
        raw_value=results.formulaic_transitions_count,
        recommendation="Reduce formulaic transitions, vary heading depth" if structure_val < 0.75 else None
    )

    # Technical Depth (6 points)
    technical_val = score_map[results.technical_score]
    technical_score = ScoreDimension(
        name="Technical Depth",
        score=technical_val * 6,
        max_score=6.0,
        percentage=technical_val * 100,
        impact=_calculate_impact(technical_val, 6.0),
        gap=(1.0 - technical_val) * 6,
        raw_value=results.domain_terms_count,
        recommendation="Increase domain-specific terminology" if technical_val < 0.75 else None
    )

    # Bold/Italic Patterns (6 points)
    bold_italic_val = score_map[results.bold_italic_score]
    bold_italic_score = ScoreDimension(
        name="Bold/Italic Patterns",
        score=bold_italic_val * 6,
        max_score=6.0,
        percentage=bold_italic_val * 100,
        impact=_calculate_impact(bold_italic_val, 6.0),
        gap=(1.0 - bold_italic_val) * 6,
        raw_value=results.bold_per_1k_words,
        recommendation="Reduce bold density to 1-5 per 1k words" if bold_italic_val < 0.75 else None
    )

    # List Usage Patterns (4 points)
    list_usage_val = score_map[results.list_usage_score]
    list_usage_score = ScoreDimension(
        name="List Usage Patterns",
        score=list_usage_val * 4,
        max_score=4.0,
        percentage=list_usage_val * 100,
        impact=_calculate_impact(list_usage_val, 4.0),
        gap=(1.0 - list_usage_val) * 4,
        raw_value=results.total_list_items,
        recommendation="Adjust list patterns and distribution" if list_usage_val < 0.75 else None
    )

    # Punctuation Clustering (4 points)
    punctuation_val = score_map[results.punctuation_score]
    punctuation_score = ScoreDimension(
        name="Punctuation Clustering",
        score=punctuation_val * 4,
        max_score=4.0,
        percentage=punctuation_val * 100,
        impact=_calculate_impact(punctuation_val, 4.0),
        gap=(1.0 - punctuation_val) * 4,
        raw_value=results.em_dash_cascading_score,
        recommendation="Break punctuation patterns (em-dash cascading, Oxford consistency)" if punctuation_val < 0.75 else None
    )

    # Whitespace Patterns (4 points)
    whitespace_val = score_map[results.whitespace_score]
    whitespace_score = ScoreDimension(
        name="Whitespace Patterns",
        score=whitespace_val * 4,
        max_score=4.0,
        percentage=whitespace_val * 100,
        impact=_calculate_impact(whitespace_val, 4.0),
        gap=(1.0 - whitespace_val) * 4,
        raw_value=results.paragraph_uniformity_score,
        recommendation="Increase paragraph length variation" if whitespace_val < 0.75 else None
    )

    # Heading Hierarchy (2 points)
    heading_hierarchy_val = score_map[results.heading_hierarchy_score]
    heading_hierarchy_score = ScoreDimension(
        name="Heading Hierarchy",
        score=heading_hierarchy_val * 2,
        max_score=2.0,
        percentage=heading_hierarchy_val * 100,
        impact=_calculate_impact(heading_hierarchy_val, 2.0),
        gap=(1.0 - heading_hierarchy_val) * 2,
        raw_value=results.heading_strict_adherence,
        recommendation="Add occasional hierarchy skips (H2→H4)" if heading_hierarchy_val < 0.75 else None
    )

    core_category = ScoreCategory(
        name="Core Patterns",
        total=(burst_score.score + perp_score.score + format_score.score + voice_score.score +
               structure_score.score + technical_score.score + bold_italic_score.score +
               list_usage_score.score + punctuation_score.score + whitespace_score.score +
               heading_hierarchy_score.score),
        max_total=74.0,
        percentage=((burst_score.score + perp_score.score + format_score.score + voice_score.score +
                    structure_score.score + technical_score.score + bold_italic_score.score +
                    list_usage_score.score + punctuation_score.score + whitespace_score.score +
                    heading_hierarchy_score.score) / 74.0) * 100,
        dimensions=[burst_score, perp_score, format_score, voice_score, structure_score,
                   technical_score, bold_italic_score, list_usage_score, punctuation_score,
                   whitespace_score, heading_hierarchy_score]
    )

    # ============================================================================
    # TIER 3: SUPPORTING INDICATORS (46 points)
    # ============================================================================

    # Lexical Diversity - Basic (6 points)
    lexical_div_val = min(1.0, results.lexical_diversity / 0.50) if results.lexical_diversity else 0.5
    lexical_div_score = ScoreDimension(
        name="Lexical Diversity (Basic)",
        score=lexical_div_val * 6,
        max_score=6.0,
        percentage=lexical_div_val * 100,
        impact=_calculate_impact(lexical_div_val, 6.0),
        gap=(1.0 - lexical_div_val) * 6,
        raw_value=results.lexical_diversity,
        recommendation="Increase vocabulary richness (target: >0.50)" if lexical_div_val < 0.75 else None
    )

    # MTLD Score (6 points)
    mtld_val = 0.5  # Default
    if results.mtld_score:
        if results.mtld_score >= 160:
            mtld_val = 1.0
        elif results.mtld_score >= 120:
            mtld_val = 0.75
        elif results.mtld_score >= 80:
            mtld_val = 0.5
        else:
            mtld_val = 0.25
    mtld_dim = ScoreDimension(
        name="MTLD (Vocabulary Variation)",
        score=mtld_val * 6,
        max_score=6.0,
        percentage=mtld_val * 100,
        impact=_calculate_impact(mtld_val, 6.0),
        gap=(1.0 - mtld_val) * 6,
        raw_value=results.mtld_score,
        recommendation="Improve MTLD score (target: ≥160)" if mtld_val < 0.75 else None
    )

    # Syntactic Repetition (4 points)
    syntactic_rep_val = 0.5  # Default
    if results.syntactic_repetition_score is not None:
        if results.syntactic_repetition_score <= 0.010:
            syntactic_rep_val = 1.0
        elif results.syntactic_repetition_score <= 0.015:
            syntactic_rep_val = 0.75
        elif results.syntactic_repetition_score <= 0.020:
            syntactic_rep_val = 0.5
        else:
            syntactic_rep_val = 0.25
    syntactic_rep_dim = ScoreDimension(
        name="Syntactic Repetition",
        score=syntactic_rep_val * 4,
        max_score=4.0,
        percentage=syntactic_rep_val * 100,
        impact=_calculate_impact(syntactic_rep_val, 4.0),
        gap=(1.0 - syntactic_rep_val) * 4,
        raw_value=results.syntactic_repetition_score,
        recommendation="Reduce structural repetition (target: ≤0.010)" if syntactic_rep_val < 0.75 else None
    )

    # Paragraph CV (6 points)
    para_cv_val = 0.5  # Default
    if results.paragraph_cv_assessment == "EXCELLENT":
        para_cv_val = 1.0
    elif results.paragraph_cv_assessment == "GOOD":
        para_cv_val = 0.75
    elif results.paragraph_cv_assessment == "FAIR":
        para_cv_val = 0.42
    elif results.paragraph_cv_assessment == "POOR":
        para_cv_val = 0.25
    para_cv_dim = ScoreDimension(
        name="Paragraph Length Variance",
        score=para_cv_val * 6,
        max_score=6.0,
        percentage=para_cv_val * 100,
        impact=_calculate_impact(para_cv_val, 6.0),
        gap=(1.0 - para_cv_val) * 6,
        raw_value=results.paragraph_cv,
        recommendation="Increase paragraph length variation (CV target: >0.60)" if para_cv_val < 0.75 else None
    )

    # Section Variance (6 points)
    section_var_val = 0.5  # Default
    if results.section_variance_assessment == "EXCELLENT":
        section_var_val = 1.0
    elif results.section_variance_assessment == "GOOD":
        section_var_val = 0.75
    elif results.section_variance_assessment == "FAIR":
        section_var_val = 0.42
    elif results.section_variance_assessment in ["POOR", "INSUFFICIENT_DATA"]:
        section_var_val = 0.25
    section_var_dim = ScoreDimension(
        name="H2 Section Length Variance",
        score=section_var_val * 6,
        max_score=6.0,
        percentage=section_var_val * 100,
        impact=_calculate_impact(section_var_val, 6.0),
        gap=(1.0 - section_var_val) * 6,
        raw_value=results.section_variance_pct,
        recommendation="Vary H2 section lengths (target: 40%+ variance)" if section_var_val < 0.75 else None
    )

    # List Depth (4 points)
    list_depth_val = 0.5  # Default
    if results.list_depth_assessment == "EXCELLENT":
        list_depth_val = 1.0
    elif results.list_depth_assessment == "GOOD":
        list_depth_val = 0.75
    elif results.list_depth_assessment == "FAIR":
        list_depth_val = 0.42
    elif results.list_depth_assessment == "POOR":
        list_depth_val = 0.25
    list_depth_dim = ScoreDimension(
        name="List Nesting Depth",
        score=list_depth_val * 4,
        max_score=4.0,
        percentage=list_depth_val * 100,
        impact=_calculate_impact(list_depth_val, 4.0),
        gap=(1.0 - list_depth_val) * 4,
        raw_value=results.list_max_depth,
        recommendation="Adjust list nesting (target: 2-3 levels)" if list_depth_val < 0.75 else None
    )

    # H3/H4 Subsection Asymmetry (6 points)
    h3_val = 0.5  # Default
    if results.subsection_assessment == "EXCELLENT":
        h3_val = 1.0
    elif results.subsection_assessment == "GOOD":
        h3_val = 0.75
    elif results.subsection_assessment == "FAIR":
        h3_val = 0.42
    elif results.subsection_assessment in ["POOR", "INSUFFICIENT_DATA"]:
        h3_val = 0.25

    h4_val = 0.5  # Default
    if results.h4_assessment == "EXCELLENT":
        h4_val = 1.0
    elif results.h4_assessment == "GOOD":
        h4_val = 0.75
    elif results.h4_assessment == "FAIR":
        h4_val = 0.42
    elif results.h4_assessment in ["POOR", "INSUFFICIENT_DATA"]:
        h4_val = 0.25

    subsection_asym_val = (h3_val + h4_val) / 2
    subsection_asym_dim = ScoreDimension(
        name="H3/H4 Subsection Asymmetry",
        score=subsection_asym_val * 6,
        max_score=6.0,
        percentage=subsection_asym_val * 100,
        impact=_calculate_impact(subsection_asym_val, 6.0),
        gap=(1.0 - subsection_asym_val) * 6,
        raw_value=results.subsection_cv,
        recommendation="Vary subsection counts (break uniform 3-4 pattern)" if subsection_asym_val < 0.75 else None
    )

    # Heading Length Variance (4 points)
    heading_length_val = 0.5  # Default
    if results.heading_length_assessment == "EXCELLENT":
        heading_length_val = 1.0
    elif results.heading_length_assessment == "GOOD":
        heading_length_val = 0.75
    elif results.heading_length_assessment == "FAIR":
        heading_length_val = 0.42
    elif results.heading_length_assessment == "POOR":
        heading_length_val = 0.25
    heading_length_dim = ScoreDimension(
        name="Heading Length Variance",
        score=heading_length_val * 4,
        max_score=4.0,
        percentage=heading_length_val * 100,
        impact=_calculate_impact(heading_length_val, 4.0),
        gap=(1.0 - heading_length_val) * 4,
        raw_value=getattr(results, 'heading_length_variance', None),
        recommendation="Vary heading lengths (avoid uniform word counts)" if heading_length_val < 0.75 else None
    )

    # Heading Depth Navigation (4 points)
    heading_depth_val = 0.5  # Default
    if results.heading_depth_assessment == "VARIED":
        heading_depth_val = 1.0
    elif results.heading_depth_assessment == "SEQUENTIAL":
        heading_depth_val = 0.5
    elif results.heading_depth_assessment == "RIGID":
        heading_depth_val = 0.25
    heading_depth_dim = ScoreDimension(
        name="Heading Depth Navigation",
        score=heading_depth_val * 4,
        max_score=4.0,
        percentage=heading_depth_val * 100,
        impact=_calculate_impact(heading_depth_val, 4.0),
        gap=(1.0 - heading_depth_val) * 4,
        raw_value=getattr(results, 'heading_depth_transitions', None),
        recommendation="Add lateral moves (H3→H3) and occasional jumps" if heading_depth_val < 0.75 else None
    )

    supporting_category = ScoreCategory(
        name="Supporting Indicators",
        total=(lexical_div_score.score + mtld_dim.score + syntactic_rep_dim.score +
               para_cv_dim.score + section_var_dim.score + list_depth_dim.score +
               subsection_asym_dim.score + heading_length_dim.score + heading_depth_dim.score),
        max_total=46.0,
        percentage=((lexical_div_score.score + mtld_dim.score + syntactic_rep_dim.score +
                    para_cv_dim.score + section_var_dim.score + list_depth_dim.score +
                    subsection_asym_dim.score + heading_length_dim.score + heading_depth_dim.score) / 46.0) * 100,
        dimensions=[lexical_div_score, mtld_dim, syntactic_rep_dim, para_cv_dim, section_var_dim,
                   list_depth_dim, subsection_asym_dim, heading_length_dim, heading_depth_dim]
    )

    # ============================================================================
    # TIER 4: PHASE 3 ADVANCED (10 points) - AST-based patterns
    # ============================================================================

    # Blockquote Clustering (3 points)
    blockquote_val = 0.5  # Default
    if results.blockquote_assessment == "EXCELLENT":
        blockquote_val = 1.0
    elif results.blockquote_assessment == "GOOD":
        blockquote_val = 0.75
    elif results.blockquote_assessment == "FAIR":
        blockquote_val = 0.42
    elif results.blockquote_assessment == "POOR":
        blockquote_val = 0.25
    blockquote_dim = ScoreDimension(
        name="Blockquote Distribution",
        score=blockquote_val * 3 if results.blockquote_assessment else 1.5,  # Neutral if not analyzed
        max_score=3.0,
        percentage=blockquote_val * 100 if results.blockquote_assessment else 50,
        impact=_calculate_impact(blockquote_val, 3.0),
        gap=(1.0 - blockquote_val) * 3 if results.blockquote_assessment else 1.5,
        raw_value=results.blockquote_score,
        recommendation="Reduce section-start clustering" if blockquote_val < 0.75 and results.blockquote_assessment else None
    )

    # Link Anchor Patterns (2 points)
    link_anchor_val = 0.5  # Default
    if results.link_anchor_assessment == "EXCELLENT":
        link_anchor_val = 1.0
    elif results.link_anchor_assessment == "GOOD":
        link_anchor_val = 0.75
    elif results.link_anchor_assessment == "FAIR":
        link_anchor_val = 0.42
    elif results.link_anchor_assessment == "POOR":
        link_anchor_val = 0.25
    link_anchor_dim = ScoreDimension(
        name="Link Anchor Text",
        score=link_anchor_val * 2 if results.link_anchor_assessment else 1.0,  # Neutral if not analyzed
        max_score=2.0,
        percentage=link_anchor_val * 100 if results.link_anchor_assessment else 50,
        impact=_calculate_impact(link_anchor_val, 2.0),
        gap=(1.0 - link_anchor_val) * 2 if results.link_anchor_assessment else 1.0,
        raw_value=results.link_anchor_score,
        recommendation="Replace generic anchors with descriptive text" if link_anchor_val < 0.75 and results.link_anchor_assessment else None
    )

    # Punctuation Spacing (2 points)
    punct_spacing_val = 0.5  # Default
    if results.punctuation_spacing_assessment == "EXCELLENT":
        punct_spacing_val = 1.0
    elif results.punctuation_spacing_assessment == "GOOD":
        punct_spacing_val = 0.75
    elif results.punctuation_spacing_assessment == "FAIR":
        punct_spacing_val = 0.42
    elif results.punctuation_spacing_assessment == "POOR":
        punct_spacing_val = 0.25
    punct_spacing_dim = ScoreDimension(
        name="Punctuation Spacing",
        score=punct_spacing_val * 2 if results.punctuation_spacing_assessment else 1.0,  # Neutral if not analyzed
        max_score=2.0,
        percentage=punct_spacing_val * 100 if results.punctuation_spacing_assessment else 50,
        impact=_calculate_impact(punct_spacing_val, 2.0),
        gap=(1.0 - punct_spacing_val) * 2 if results.punctuation_spacing_assessment else 1.0,
        raw_value=results.punctuation_spacing_score,
        recommendation="Vary punctuation spacing patterns" if punct_spacing_val < 0.75 and results.punctuation_spacing_assessment else None
    )

    # List AST Symmetry (2 points)
    list_ast_val = 0.5  # Default
    if results.list_ast_assessment == "EXCELLENT":
        list_ast_val = 1.0
    elif results.list_ast_assessment == "GOOD":
        list_ast_val = 0.75
    elif results.list_ast_assessment == "FAIR":
        list_ast_val = 0.42
    elif results.list_ast_assessment == "POOR":
        list_ast_val = 0.25
    list_ast_dim = ScoreDimension(
        name="List Symmetry (AST)",
        score=list_ast_val * 2 if results.list_ast_assessment else 1.0,  # Neutral if not analyzed
        max_score=2.0,
        percentage=list_ast_val * 100 if results.list_ast_assessment else 50,
        impact=_calculate_impact(list_ast_val, 2.0),
        gap=(1.0 - list_ast_val) * 2 if results.list_ast_assessment else 1.0,
        raw_value=results.list_ast_score,
        recommendation="Break symmetric list patterns" if list_ast_val < 0.75 and results.list_ast_assessment else None
    )

    # Code AST Patterns (1 point)
    code_ast_val = 0.5  # Default
    if results.code_ast_assessment == "EXCELLENT":
        code_ast_val = 1.0
    elif results.code_ast_assessment == "GOOD":
        code_ast_val = 0.75
    elif results.code_ast_assessment == "FAIR":
        code_ast_val = 0.42
    elif results.code_ast_assessment == "POOR":
        code_ast_val = 0.25
    code_ast_dim = ScoreDimension(
        name="Code Block Patterns",
        score=code_ast_val * 1 if results.code_ast_assessment else 0.5,  # Neutral if not analyzed
        max_score=1.0,
        percentage=code_ast_val * 100 if results.code_ast_assessment else 50,
        impact=_calculate_impact(code_ast_val, 1.0),
        gap=(1.0 - code_ast_val) * 1 if results.code_ast_assessment else 0.5,
        raw_value=results.code_ast_score,
        recommendation="Improve code block language declarations" if code_ast_val < 0.75 and results.code_ast_assessment else None
    )

    phase3_category = ScoreCategory(
        name="Advanced Structural Patterns",
        total=(blockquote_dim.score + link_anchor_dim.score + punct_spacing_dim.score +
               list_ast_dim.score + code_ast_dim.score),
        max_total=10.0,
        percentage=((blockquote_dim.score + link_anchor_dim.score + punct_spacing_dim.score +
                    list_ast_dim.score + code_ast_dim.score) / 10.0) * 100,
        dimensions=[blockquote_dim, link_anchor_dim, punct_spacing_dim, list_ast_dim, code_ast_dim]
    )

    # ============================================================================
    # CALCULATE OVERALL SCORES
    # ============================================================================

    total_score = (advanced_category.total + core_category.total +
                   supporting_category.total + phase3_category.total)
    total_possible = (advanced_category.max_total + core_category.max_total +
                     supporting_category.max_total + phase3_category.max_total)

    quality_score = (total_score / total_possible * 100) if total_possible > 0 else 0
    detection_risk = 100 - quality_score  # Inverse relationship

    # Interpretations
    quality_interp = _interpret_quality(quality_score)
    detection_interp = _interpret_detection(detection_risk)

    # Gaps
    quality_gap = max(0, quality_target - quality_score)
    detection_gap = max(0, detection_risk - detection_target)

    # ============================================================================
    # GENERATE IMPROVEMENT ACTIONS
    # ============================================================================

    all_dimensions = (
        advanced_category.dimensions +
        core_category.dimensions +
        supporting_category.dimensions +
        phase3_category.dimensions
    )

    improvements = []
    for dim in all_dimensions:
        if dim.gap > 0.5:  # Only suggest if gap > 0.5 points
            improvements.append(ImprovementAction(
                priority=0,  # Will be set later
                dimension=dim.name,
                current_score=dim.score,
                max_score=dim.max_score,
                potential_gain=dim.gap,
                impact_level=dim.impact,
                action=dim.recommendation or f"Improve {dim.name}",
                effort_level=_estimate_effort(dim.name, dim.gap)
            ))

    # Sort by ROI (gain / effort)
    effort_multiplier = {'LOW': 1.0, 'MEDIUM': 0.7, 'HIGH': 0.4}
    improvements.sort(key=lambda x: x.potential_gain * effort_multiplier[x.effort_level], reverse=True)

    # Set priorities
    for i, imp in enumerate(improvements, start=1):
        imp.priority = i

    # Path to target - actions needed to reach quality target
    path = []
    cumulative_gain = quality_score
    for imp in improvements:
        if cumulative_gain >= quality_target:
            break
        path.append(imp)
        cumulative_gain += imp.potential_gain

    # Estimate overall effort
    if quality_gap == 0:
        effort = "MINIMAL"
    elif quality_gap < 5:
        effort = "LIGHT"
    elif quality_gap < 15:
        effort = "MODERATE"
    elif quality_gap < 30:
        effort = "SUBSTANTIAL"
    else:
        effort = "EXTENSIVE"

    return DualScore(
        detection_risk=round(detection_risk, 1),
        quality_score=round(quality_score, 1),
        detection_interpretation=detection_interp,
        quality_interpretation=quality_interp,
        detection_target=detection_target,
        quality_target=quality_target,
        detection_gap=round(detection_gap, 1),
        quality_gap=round(quality_gap, 1),
        categories=[advanced_category, core_category, supporting_category, phase3_category],
        improvements=improvements,
        path_to_target=path,
        estimated_effort=effort,
        timestamp=timestamp,
        file_path=results.file_path,
        total_words=results.total_words
    )


def _calculate_impact(current_val: float, max_points: float) -> str:
    """Calculate impact level based on gap and point weight."""
    gap = (1.0 - current_val) * max_points
    if gap < 1.0:
        return "NONE"
    elif gap < 2.0:
        return "LOW"
    elif gap < 4.0:
        return "MEDIUM"
    else:
        return "HIGH"


def _estimate_effort(dimension_name: str, gap: float) -> str:
    """Estimate effort required to close gap."""
    # Some dimensions are easier to fix than others
    easy_fixes = [
        "Formatting Patterns", "Stylometric Markers", "Heading Hierarchy",
        "Bold/Italic Patterns", "Punctuation Clustering", "Whitespace Patterns",
        "Blockquote Distribution", "Link Anchor Text", "Punctuation Spacing",
        "List Symmetry (AST)", "Code Block Patterns"
    ]
    medium_fixes = [
        "Burstiness (Sentence Variation)", "Perplexity (Vocabulary)",
        "Structure & Organization", "Voice & Authenticity", "Technical Depth",
        "List Usage Patterns", "List Nesting Depth", "Heading Length Variance",
        "Heading Depth Navigation", "Paragraph Length Variance",
        "H2 Section Length Variance", "H3/H4 Subsection Asymmetry"
    ]
    hard_fixes = [
        "GLTR Token Ranking", "Advanced Lexical (HDD/Yule's K)",
        "Syntactic Complexity", "MATTR (Lexical Richness)",
        "RTTR (Global Diversity)", "AI Detection Ensemble",
        "Multi-Model Perplexity Consensus", "Lexical Diversity (Basic)",
        "MTLD (Vocabulary Variation)", "Syntactic Repetition"
    ]

    if dimension_name in easy_fixes:
        return "LOW" if gap < 3 else "MEDIUM"
    elif dimension_name in medium_fixes:
        return "MEDIUM" if gap < 4 else "HIGH"
    else:  # hard_fixes or unknown
        return "HIGH"


def _interpret_quality(score: float) -> str:
    """Interpret quality score."""
    if score >= 95:
        return "EXCEPTIONAL - Indistinguishable from human"
    elif score >= 85:
        return "EXCELLENT - Minimal AI signatures"
    elif score >= 70:
        return "GOOD - Natural with minor tells"
    elif score >= 50:
        return "MIXED - Needs moderate work"
    elif score >= 30:
        return "AI-LIKE - Substantial work needed"
    else:
        return "OBVIOUS AI - Complete rewrite"


def _interpret_detection(risk: float) -> str:
    """Interpret detection risk."""
    if risk >= 70:
        return "VERY HIGH - Will be flagged"
    elif risk >= 50:
        return "HIGH - Likely flagged"
    elif risk >= 30:
        return "MEDIUM - May be flagged"
    elif risk >= 15:
        return "LOW - Unlikely flagged"
    else:
        return "VERY LOW - Safe"
