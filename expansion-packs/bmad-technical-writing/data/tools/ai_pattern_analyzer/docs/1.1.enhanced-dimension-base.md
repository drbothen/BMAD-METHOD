# Story 1.1: Enhanced Dimension Base Contract

## Status
Ready for Review

## Story
**As a** dimension developer,
**I want** dimensions to declare all metadata (weight, tiers, recommendations),
**so that** the algorithm can dynamically incorporate them without core code modifications.

## Acceptance Criteria
1. New `DimensionStrategy` ABC created in `dimensions/base_strategy.py`
2. Must include all required abstract methods and properties
3. Full type hints on all methods
4. Comprehensive docstrings with examples
5. No breaking changes to existing dimensions yet
6. Maintains backward compatibility with existing `DimensionAnalyzer` base class
7. Includes AST helper methods from existing base.py
8. Provides validation helpers for tier, weight, and score values

## Tasks / Subtasks

### Core Base Class Implementation
- [x] Create `dimensions/base_strategy.py` with full implementation (AC: 1, 2)
  - [x] Import required dependencies (abc, typing, enum, marko)
  - [x] Define abstract base class `DimensionStrategy`
  - [x] Add `_markdown_parser` and `_ast_cache` instance variables in __init__
  - [x] Implement `dimension_name` property (abstract) → str
  - [x] Implement `weight` property (abstract) → float (0-100 scale)
  - [x] Implement `tier` property (abstract) → DimensionTier enum
  - [x] Implement `description` property (abstract) → str
  - [x] Implement `analyze(text, lines, **kwargs)` method (abstract) → Dict[str, Any]
  - [x] Implement `calculate_score(metrics)` method (abstract) → float (0-100)
  - [x] Implement `get_recommendations(score, metrics)` method (abstract) → List[str]
  - [x] Implement `get_tiers()` method (abstract) → Dict[str, Tuple[float, float]]
  - [x] Implement `get_impact_level(score)` method (concrete with default logic) → str

### Backward Compatibility Methods
- [x] Add backward compatibility support (AC: 5, 6)
  - [x] Implement concrete `score(analysis_results)` method wrapping calculate_score() → Tuple[float, str]
  - [x] Implement `_map_score_to_tier(score)` helper method
  - [x] Implement `analyze_detailed(lines, html_comment_checker)` method (concrete, optional override) → List[Any]
  - [x] Add `get_max_score()` deprecated method → float (always returns 100.0)
  - [x] Add `get_dimension_name()` deprecated method → str (returns dimension_name property)

### AST Helper Methods (from existing base.py)
- [x] Include AST helper methods from existing base.py (AC: 7)
  - [x] Implement `_get_markdown_parser()` - lazy load marko parser
  - [x] Implement `_parse_to_ast(text, cache_key)` - parse markdown to AST with caching
  - [x] Implement `_walk_ast(node, node_type)` - recursive AST traversal
  - [x] Implement `_extract_text_from_node(node)` - extract plain text from AST node

### Enums and Validation
- [x] Create DimensionTier enum (AC: 8)
  - [x] Define DimensionTier(str, Enum) with ADVANCED, CORE, SUPPORTING, STRUCTURAL
  - [x] Add docstrings explaining each tier's purpose

- [x] Add validation helpers (AC: 8)
  - [x] Create `_validate_tier()` method - validates tier is valid DimensionTier
  - [x] Create `_validate_score(score)` method - validates score is 0-100
  - [x] Create `_validate_weight(weight)` method - validates weight is 0-100
  - [x] Create `_calculate_gap(score)` static method - calculates gap from 100

### Documentation
- [x] Add comprehensive docstrings with examples (AC: 4)
  - [x] Class-level docstring with complete usage example
  - [x] Docstring for each abstract property with type and description
  - [x] Docstring for each abstract method with args, returns, raises
  - [x] Docstring for each concrete method with default behavior explanation
  - [x] Include parameter descriptions and return types for all methods
  - [x] Document common kwargs for analyze() method (word_count, domain, file_path)
  - [x] Add module-level docstring explaining the base contract

### Type Hints
- [x] Include full type hints on all methods/properties (AC: 3)
  - [x] Import typing modules (Dict, List, Tuple, Any, Optional, Final)
  - [x] Add return type hints to all methods
  - [x] Add parameter type hints to all methods
  - [x] Use DimensionTier enum for tier property type
  - [x] Use proper generic types (Dict[str, Any], List[str], etc.)

### Module Updates
- [x] Update `dimensions/__init__.py` to export new base class (AC: 1)
  - [x] Add `from .base_strategy import DimensionStrategy`
  - [x] Add `from .base_strategy import DimensionTier`
  - [x] Update __all__ list to include both exports
  - [x] Keep existing DimensionAnalyzer export for backward compatibility

### Testing
- [x] Write unit tests for base class methods (AC: 5)
  - [x] Test that base class cannot be instantiated (TypeError)
  - [x] Test abstract method enforcement (cannot create incomplete subclass)
  - [x] Test tier validation with valid DimensionTier values
  - [x] Test tier validation with invalid tier values (raises ValueError)
  - [x] Test score validation (0-100 range, raises ValueError outside range)
  - [x] Test weight validation (0-100 range, raises ValueError outside range)
  - [x] Test get_impact_level() logic with all threshold boundaries
  - [x] Test _calculate_gap() static method
  - [x] Test AST helper methods (_get_markdown_parser, _parse_to_ast, _walk_ast, _extract_text_from_node)
  - [x] Test backward compatibility methods (score, get_max_score, get_dimension_name)
  - [x] Test analyze_detailed() default implementation returns empty list
  - [x] Test _map_score_to_tier() with various score values

- [x] Write integration test with existing base class (AC: 6)
  - [x] Verify both DimensionAnalyzer and DimensionStrategy can coexist
  - [x] Test that legacy dimensions using old base still work
  - [x] Test that new dimensions using new base work correctly
  - [x] Verify no namespace conflicts

## Dev Notes

### Architecture Context
This story is the foundation for the self-registering dimension architecture refactoring. It creates the enhanced base contract that all dimensions will eventually implement.

**CRITICAL:** This base class must maintain **backward compatibility** with the existing `DimensionAnalyzer` base class during the transition period. Both base classes will coexist until all dimensions are migrated (Story 1.4).

### Current System Integration
- **Integrates with**: Existing `dimensions/base.py` (DimensionAnalyzer class with analyze() and score() methods)
- **Technology**: Python 3.10+, ABC (Abstract Base Classes), marko (Markdown AST parsing)
- **Follows pattern**: Abstract base class pattern for plugin architecture
- **Touch points**: All 10 existing dimensions will eventually inherit from this
- **Transition strategy**: New base coexists with old base until migration complete (Stories 1.1-1.4)

### Key Design Patterns
- **Strategy Pattern**: Each dimension represents a different analysis strategy that can be swapped dynamically
- **Template Method**: Base class defines the interface contract that subclasses must implement
- **Adapter Pattern**: score() method adapts new calculate_score() to legacy interface for backward compatibility

### Implementation Details

The new `DimensionStrategy` class must include:

#### Abstract Properties (must override in subclasses)

**`dimension_name: str`**
- Unique dimension identifier (lowercase, alphanumeric + underscores)
- Example: `"perplexity"`, `"burstiness"`, `"code_block"`
- Used as registry key

**`weight: float`**
- Contribution weight for overall scoring (0-100 scale)
- Sum across all dimensions must equal 100.0 (validated by WeightMediator in Story 1.3)
- Example: `5.0` means 5% of total score

**`tier: DimensionTier`**
- Grouping tier enum (ADVANCED, CORE, SUPPORTING, STRUCTURAL)
- Used for organizing dimensions in reports
- Each tier has target weight range (see Valid Tier Values below)

**`description: str`**
- Human-readable description of what this dimension analyzes
- Example: `"Analyzes vocabulary predictability and AI-typical word patterns"`

#### Abstract Methods (must implement in subclasses)

**`analyze(text: str, lines: List[str], **kwargs) -> Dict[str, Any]`**
- Perform dimension-specific analysis on text
- **Parameters:**
  - `text`: Full document text
  - `lines`: Text split into lines (for line-by-line analysis)
  - `**kwargs`: Additional parameters (see Standard kwargs below)
- **Returns:** Dict with dimension-specific metrics
- **Example return:**
  ```python
  {
      'ai_vocabulary': {'count': 5, 'per_1k': 2.3, 'words': ['delve', 'leverage']},
      'formulaic_transitions': {'count': 3, 'transitions': ['Furthermore,', 'Moreover,']}
  }
  ```

**`calculate_score(metrics: Dict[str, Any]) -> float`**
- Convert raw metrics from analyze() into 0-100 score
- **Parameters:**
  - `metrics`: Output from analyze() method
- **Returns:** Score from 0.0 (AI-like) to 100.0 (human-like)
- **Must call:** `self._validate_score(score)` before returning
- **Example:**
  ```python
  def calculate_score(self, metrics: Dict[str, Any]) -> float:
      ai_vocab_per_1k = metrics.get('ai_vocabulary', {}).get('per_1k', 0)

      if ai_vocab_per_1k < 1.0:
          score = 100.0
      elif ai_vocab_per_1k < 3.0:
          score = 75.0
      else:
          score = 50.0

      self._validate_score(score)
      return score
  ```

**`get_recommendations(score: float, metrics: Dict[str, Any]) -> List[str]`**
- Generate actionable recommendations based on score and metrics
- **Parameters:**
  - `score`: The calculated score from calculate_score()
  - `metrics`: Raw metrics from analyze() for detailed recommendations
- **Returns:** List of recommendation strings
- **Example:**
  ```python
  [
      "Reduce AI vocabulary from 5.2 to <1.0 per 1k words",
      "Replace 'delve' with: explore, examine, investigate"
  ]
  ```

**`get_tiers() -> Dict[str, Tuple[float, float]]`**
- Define score tier ranges for interpretation
- **Returns:** Dict mapping tier name to (min_score, max_score) range
- **Standard tiers:**
  ```python
  {
      'excellent': (90.0, 100.0),
      'good': (75.0, 89.9),
      'acceptable': (50.0, 74.9),
      'poor': (0.0, 49.9)
  }
  ```

#### Concrete Methods (can optionally override in subclasses)

**`get_impact_level(score: float) -> str`**
- Calculate impact level based on score gap from 100 (perfect)
- **Default logic provided** - subclasses may override for custom thresholds
- **Parameters:**
  - `score`: The score to evaluate (0-100)
- **Returns:** "NONE", "LOW", "MEDIUM", or "HIGH"
- **Default implementation:**
  ```python
  gap = 100.0 - score
  if gap < 5:   return "NONE"     # 95-100: minimal impact
  if gap < 15:  return "LOW"      # 85-94: low impact
  if gap < 30:  return "MEDIUM"   # 70-84: medium impact
  return "HIGH"                    # 0-69: high impact
  ```
- **Override example:** Performance-critical dimensions may use stricter thresholds

**`analyze_detailed(lines: List[str], html_comment_checker=None) -> List[Any]`**
- Optional detailed analysis with line-by-line findings
- **Default implementation** returns empty list
- **Override in subclasses** to provide detailed reporting mode
- **Used by:** CLI detailed output mode
- **Returns:** List of issue objects (VocabInstance, SentenceBurstinessIssue, etc.)
- **Example override:**
  ```python
  def analyze_detailed(self, lines, html_comment_checker=None):
      issues = []
      for line_num, line in enumerate(lines, start=1):
          # Detailed line-by-line analysis
          if self._has_issue(line):
              issues.append(IssueObject(line_number=line_num, ...))
      return issues
  ```

**`score(analysis_results: Dict[str, Any]) -> Tuple[float, str]`**
- **Legacy compatibility wrapper** for old base class interface
- Calls calculate_score() and maps to tier label
- **DO NOT override** - override calculate_score() instead
- **Returns:** Tuple of (score_value, score_label)
- **Implementation:**
  ```python
  def score(self, analysis_results: Dict[str, Any]) -> Tuple[float, str]:
      score_value = self.calculate_score(analysis_results)
      tier_label = self._map_score_to_tier(score_value)
      return (score_value, tier_label)
  ```

#### AST Helper Methods (inherited, protected)

These methods are available to all subclasses for markdown structure analysis:

**`_get_markdown_parser() -> Markdown`**
- Lazy-load marko parser singleton
- **Returns:** marko.Markdown instance
- **Usage:** `parser = self._get_markdown_parser()`

**`_parse_to_ast(text: str, cache_key: Optional[str] = None) -> Any`**
- Parse markdown to AST with caching
- **Parameters:**
  - `text`: Markdown text to parse
  - `cache_key`: Optional cache key for reusing parsed AST
- **Returns:** Parsed AST node or None if parsing fails
- **Usage:** `ast = self._parse_to_ast(text, cache_key='main')`

**`_walk_ast(node, node_type=None) -> List`**
- Recursively traverse AST for nodes of specified type
- **Parameters:**
  - `node`: AST node to walk
  - `node_type`: Optional type to filter (e.g., marko.block.Quote)
- **Returns:** List of nodes matching node_type, or all nodes if node_type is None
- **Usage:** `quotes = self._walk_ast(ast, marko.block.Quote)`

**`_extract_text_from_node(node) -> str`**
- Extract plain text from AST node recursively
- **Parameters:**
  - `node`: AST node to extract text from
- **Returns:** Plain text string
- **Usage:** `text = self._extract_text_from_node(heading_node)`

#### Validation Helper Methods (protected)

**`_validate_tier() -> None`**
- Validate tier property value
- **Raises:** ValueError if tier is not valid DimensionTier
- **Usage:** Call in __init__ after setting tier

**`_validate_score(score: float) -> None`**
- Validate score is in 0-100 range
- **Raises:** ValueError if score outside range
- **Usage:** Call in calculate_score() before returning

**`_validate_weight(weight: float) -> None`**
- Validate weight is in 0-100 range
- **Raises:** ValueError if weight outside range
- **Usage:** Call in __init__ after setting weight

**`_calculate_gap(score: float) -> float`** (static method)
- Calculate gap from perfect score (100)
- **Parameters:** score (0-100)
- **Returns:** 100.0 - score
- **Usage:** `gap = self._calculate_gap(score)`

**`_map_score_to_tier(score: float) -> str`**
- Map score to tier label using get_tiers()
- **Parameters:** score (0-100)
- **Returns:** Tier label string (e.g., "EXCELLENT", "GOOD")
- **Usage:** Called internally by score() method

### Valid Tier Values (DimensionTier Enum)

```python
from enum import Enum

class DimensionTier(str, Enum):
    """Valid dimension tier classifications."""

    ADVANCED = "ADVANCED"      # ML-based, highest accuracy detection
    CORE = "CORE"             # Proven AI signatures
    SUPPORTING = "SUPPORTING"  # Contextual indicators
    STRUCTURAL = "STRUCTURAL"  # AST-based structural patterns
```

**Tier Descriptions and Weight Targets:**

| Tier | Description | Target Weight % | Example Dimensions |
|------|-------------|----------------|-------------------|
| **ADVANCED** | ML-based, highest accuracy metrics (95%+ on GPT detection) | 30-40% | GLTR, Stylometric, Syntactic, Multi-Model Perplexity |
| **CORE** | Proven AI signatures (>85% accuracy, established research) | 35-45% | Burstiness, Perplexity, Formatting, Voice, Structure |
| **SUPPORTING** | Contextual quality indicators (correlation with AI text) | 15-25% | Lexical Diversity, Sentiment, Technical Depth |
| **STRUCTURAL** | AST-based structural patterns (mechanical structure detection) | 5-10% | Blockquote, Link Anchor, List AST, Code AST |

**Weight Distribution Requirements:**
- All dimension weights must sum to **exactly 100.0** (±0.1% tolerance)
- Validated by WeightMediator (Story 1.3) before analysis execution
- Each tier has recommended weight range to maintain balance

### Scoring Convention

**All dimensions use the same 0-100 scale:**
- **100.0** = most human-like (perfect score, no AI patterns detected)
- **0.0** = most AI-like (worst score, strong AI patterns detected)

**This inverted scale means:**
- Higher scores are better (more human-like)
- Lower scores indicate AI-generated content
- Gap from 100 represents "room for improvement"

### analyze() Method Standard Parameters

**Required parameters:**
- `text` (str): Full document text
- `lines` (List[str]): Text split into lines

**Common kwargs (pass-through from analyzer):**
- `word_count` (int): Pre-calculated word count for efficiency (avoids re-counting)
- `domain` (DocumentDomain): Document type for threshold selection
  - Values: GENERAL, TECHNICAL, CREATIVE, ACADEMIC, BUSINESS
  - Used by StructureAnalyzer for domain-specific thresholds
- `file_path` (str, optional): File path for context/logging

**Best Practice:** Subclasses should accept `**kwargs` and extract only what they need:
```python
def analyze(self, text: str, lines: List[str], **kwargs) -> Dict[str, Any]:
    word_count = kwargs.get('word_count', len(text.split()))
    domain = kwargs.get('domain', DocumentDomain.GENERAL)
    # Use parameters as needed
```

This pattern allows **forward compatibility** when new kwargs are added.

### Backward Compatibility Requirements

**During transition period (Stories 1.1-1.4):**
1. Both `DimensionAnalyzer` (old) and `DimensionStrategy` (new) must coexist
2. New base class must support legacy `score()` method signature
3. Existing dimensions continue using `DimensionAnalyzer` until Story 1.4
4. No breaking changes to existing dimension interfaces until explicit migration

**Method Compatibility Mapping:**

| Old (DimensionAnalyzer) | New (DimensionStrategy) | Compatibility |
|------------------------|------------------------|--------------|
| `score(analysis_results) -> Tuple[float, str]` | `calculate_score(metrics) -> float` | score() wraps calculate_score() |
| `analyze(text, lines, **kwargs)` | `analyze(text, lines, **kwargs)` | Identical signature ✅ |
| `get_dimension_name() -> str` | `dimension_name` property | get_dimension_name() reads property |
| `get_max_score() -> float` | N/A (always 100.0) | Deprecated method returns 100.0 |

### Required Dependencies

**Standard Library:**
- `abc` - Abstract base classes (ABC, abstractmethod)
- `typing` - Type hints (Dict, List, Tuple, Any, Optional, Final)
- `enum` - Enumerations (Enum)

**Third-Party:**
- `marko` - Markdown AST parsing (required for AST helpers)
  - `marko.Markdown` - Main parser class
  - `marko.block` - Block-level nodes (Quote, Heading, List, Paragraph, FencedCode)
  - `marko.inline` - Inline nodes (Link, CodeSpan)

**Installation:**
```bash
pip install marko
```

### File Structure

```
dimensions/
├── base.py              # Existing base (DimensionAnalyzer) - keep for now
├── base_strategy.py     # NEW - Enhanced base contract (DimensionStrategy)
├── perplexity.py        # Existing - will migrate in Story 1.4
├── burstiness.py        # Existing - will migrate in Story 1.4
├── structure.py         # Existing - will migrate in Story 1.4
├── formatting.py        # Existing - will migrate in Story 1.4
├── voice.py             # Existing - will migrate in Story 1.4
├── syntactic.py         # Existing - will migrate in Story 1.4
├── lexical.py           # Existing - will migrate in Story 1.4
├── stylometric.py       # Existing - will migrate in Story 1.4
├── advanced.py          # Existing - will migrate in Story 1.4
├── sentiment.py         # Existing - will migrate in Story 1.4
└── __init__.py          # Export both bases during transition
```

### Complete Example Implementation

Here's a minimal but complete dimension using the new base class:

```python
# dimensions/code_block.py
"""
Code Block dimension analyzer.

Analyzes code block usage patterns - a supporting indicator for AI detection.
AI-generated content tends to use generic code blocks without language specifiers
and has uniform code block lengths.
"""

import re
from typing import Dict, List, Tuple, Any
from ai_pattern_analyzer.dimensions.base_strategy import DimensionStrategy, DimensionTier
from ai_pattern_analyzer.core.dimension_registry import DimensionRegistry


class CodeBlockDimension(DimensionStrategy):
    """
    Analyzes code block usage patterns.

    Detects:
    - Frequency of code blocks
    - Code block language distribution
    - Inline code usage
    - Code-to-text ratio

    AI text tends to:
    - Use generic code blocks without language specifiers
    - Have uniform code block lengths
    - Over-use inline code formatting
    """

    def __init__(self):
        """Initialize and compile patterns for performance."""
        super().__init__()

        # Compile regex patterns once for efficiency
        self._code_block_pattern = re.compile(r'```(\w*)\n(.*?)```', re.DOTALL)
        self._inline_code_pattern = re.compile(r'`([^`]+)`')

        # Validate properties on initialization
        self._validate_weight(self.weight)
        self._validate_tier()

    # ===== Metadata Properties =====

    @property
    def dimension_name(self) -> str:
        """Unique identifier for this dimension."""
        return "code_block"

    @property
    def weight(self) -> float:
        """Weight contribution: 2% of total score."""
        return 2.0

    @property
    def tier(self) -> DimensionTier:
        """Classification tier: SUPPORTING."""
        return DimensionTier.SUPPORTING

    @property
    def description(self) -> str:
        """Human-readable description."""
        return "Analyzes code block usage patterns and code-to-text ratios"

    # ===== Analysis Methods =====

    def analyze(self, text: str, lines: List[str], **kwargs) -> Dict[str, Any]:
        """
        Analyze code block patterns in text.

        Args:
            text: Full document text
            lines: Text split into lines
            **kwargs: Additional parameters (word_count)

        Returns:
            Dict containing code block metrics
        """
        word_count = kwargs.get('word_count', len(text.split()))

        # Find all code blocks
        code_blocks = list(self._code_block_pattern.finditer(text))
        code_block_count = len(code_blocks)

        # Analyze language specifiers
        languages = [match.group(1) for match in code_blocks if match.group(1)]
        unspecified_count = code_block_count - len(languages)

        # Find inline code
        inline_code = self._inline_code_pattern.findall(text)
        inline_code_count = len(inline_code)

        # Calculate ratios
        code_block_per_1k = (code_block_count / word_count * 1000) if word_count > 0 else 0
        inline_per_1k = (inline_code_count / word_count * 1000) if word_count > 0 else 0

        return {
            'code_blocks': {
                'total': code_block_count,
                'with_language': len(languages),
                'without_language': unspecified_count,
                'per_1k_words': code_block_per_1k,
                'languages': languages
            },
            'inline_code': {
                'total': inline_code_count,
                'per_1k_words': inline_per_1k,
                'examples': inline_code[:10]  # First 10 for debugging
            }
        }

    def calculate_score(self, metrics: Dict[str, Any]) -> float:
        """
        Calculate 0-100 score based on code metrics.

        Scoring logic:
        - Penalize missing language specifiers
        - Penalize excessive inline code
        - Reward balanced code block usage

        Args:
            metrics: Output from analyze() method

        Returns:
            Score from 0.0 (AI-like) to 100.0 (human-like)
        """
        code_blocks = metrics.get('code_blocks', {})
        inline_code = metrics.get('inline_code', {})

        score = 100.0

        # Penalty for unspecified language
        unspecified_count = code_blocks.get('without_language', 0)
        total_blocks = code_blocks.get('total', 0)
        if total_blocks > 0:
            unspecified_ratio = unspecified_count / total_blocks
            score -= unspecified_ratio * 30  # Up to -30 points

        # Penalty for excessive inline code
        inline_per_1k = inline_code.get('per_1k_words', 0)
        if inline_per_1k > 10:
            score -= min((inline_per_1k - 10) * 2, 40)  # Up to -40 points

        # Validate before returning
        score = max(0.0, min(100.0, score))
        self._validate_score(score)
        return score

    def get_recommendations(self, score: float, metrics: Dict[str, Any]) -> List[str]:
        """
        Generate actionable recommendations.

        Args:
            score: Current score from calculate_score()
            metrics: Raw metrics from analyze()

        Returns:
            List of recommendation strings
        """
        recommendations = []

        code_blocks = metrics.get('code_blocks', {})
        inline_code = metrics.get('inline_code', {})

        # Recommendation for language specifiers
        unspecified = code_blocks.get('without_language', 0)
        if unspecified > 0:
            recommendations.append(
                f"Add language specifiers to {unspecified} code blocks "
                f"(e.g., ```python, ```javascript)"
            )

        # Recommendation for inline code
        inline_per_1k = inline_code.get('per_1k_words', 0)
        if inline_per_1k > 10:
            recommendations.append(
                f"Reduce inline code usage from {inline_per_1k:.1f} to <10 per 1k words"
            )
            recommendations.append(
                "Convert repetitive inline code to code blocks"
            )

        return recommendations

    def get_tiers(self) -> Dict[str, Tuple[float, float]]:
        """
        Define score tier ranges.

        Returns:
            Dict mapping tier name to (min_score, max_score) tuple
        """
        return {
            'excellent': (90.0, 100.0),
            'good': (75.0, 89.9),
            'acceptable': (50.0, 74.9),
            'poor': (0.0, 49.9)
        }
```

### Migration Notes for Developers

When Story 1.4 migrates existing dimensions to this new base class:

1. **Change base class:**
   ```python
   # Old
   from ai_pattern_analyzer.dimensions.base import DimensionAnalyzer
   class MyDimension(DimensionAnalyzer):

   # New
   from ai_pattern_analyzer.dimensions.base_strategy import DimensionStrategy, DimensionTier
   class MyDimension(DimensionStrategy):
   ```

2. **Add metadata properties:**
   ```python
   @property
   def dimension_name(self) -> str:
       return "my_dimension"

   @property
   def weight(self) -> float:
       return 5.0

   @property
   def tier(self) -> DimensionTier:
       return DimensionTier.CORE

   @property
   def description(self) -> str:
       return "Analyzes..."
   ```

3. **Rename `score()` to `calculate_score()`:**
   ```python
   # Old
   def score(self, analysis_results: Dict[str, Any]) -> Tuple[float, str]:
       if metric < threshold:
           return (10.0, "HIGH")
       return (5.0, "MEDIUM")

   # New
   def calculate_score(self, metrics: Dict[str, Any]) -> float:
       if metric < threshold:
           return 100.0
       return 75.0
   ```

4. **Add `get_recommendations()` and `get_tiers()`:**
   ```python
   def get_recommendations(self, score: float, metrics: Dict[str, Any]) -> List[str]:
       if score < 75:
           return ["Improve X", "Adjust Y"]
       return []

   def get_tiers(self) -> Dict[str, Tuple[float, float]]:
       return {
           'excellent': (90.0, 100.0),
           'good': (75.0, 89.9),
           'acceptable': (50.0, 74.9),
           'poor': (0.0, 49.9)
       }
   ```

## Testing

### Test File Location
- Unit tests: `tests/dimensions/test_base_strategy.py`
- Integration tests: `tests/integration/test_base_compatibility.py`

### Test Standards
- Use pytest framework
- Aim for 100% coverage of the base class
- Test abstract enforcement (cannot instantiate base class)
- Test concrete methods (get_impact_level, score, analyze_detailed)
- Test validation helpers
- Test AST helper methods
- Test backward compatibility

### Testing Frameworks and Patterns
- pytest for test framework
- pytest.raises for exception testing
- Mock dimensions for testing abstract enforcement
- unittest.mock for mocking marko parser

### Specific Testing Requirements

```python
import pytest
from ai_pattern_analyzer.dimensions.base_strategy import DimensionStrategy, DimensionTier

# Test that base class is properly abstract
def test_cannot_instantiate_base():
    """Verify DimensionStrategy cannot be instantiated directly."""
    with pytest.raises(TypeError):
        DimensionStrategy()

# Test abstract method enforcement
def test_abstract_method_enforcement():
    """Verify incomplete subclass cannot be instantiated."""
    class IncompleteDimension(DimensionStrategy):
        @property
        def dimension_name(self):
            return "incomplete"
        # Missing other required methods

    with pytest.raises(TypeError):
        IncompleteDimension()

# Test tier validation with valid values
def test_tier_validation_valid():
    """Verify valid tier values are accepted."""
    class TestDimension(DimensionStrategy):
        @property
        def tier(self):
            return DimensionTier.CORE
        # ... other methods

    dim = TestDimension()
    dim._validate_tier()  # Should not raise

# Test tier validation with invalid values
def test_tier_validation_invalid():
    """Verify invalid tier values raise ValueError."""
    class TestDimension(DimensionStrategy):
        def __init__(self):
            super().__init__()
            self._tier = "INVALID_TIER"

        @property
        def tier(self):
            return self._tier
        # ... other methods

    dim = TestDimension()
    with pytest.raises(ValueError, match="tier must be one of"):
        dim._validate_tier()

# Test score validation
def test_score_validation():
    """Verify score validation enforces 0-100 range."""
    class TestDimension(DimensionStrategy):
        # ... required methods
        pass

    dim = TestDimension()

    # Valid scores
    dim._validate_score(0.0)
    dim._validate_score(50.0)
    dim._validate_score(100.0)

    # Invalid scores
    with pytest.raises(ValueError):
        dim._validate_score(-1.0)

    with pytest.raises(ValueError):
        dim._validate_score(101.0)

# Test weight validation
def test_weight_validation():
    """Verify weight validation enforces 0-100 range."""
    class TestDimension(DimensionStrategy):
        # ... required methods
        pass

    dim = TestDimension()

    # Valid weights
    dim._validate_weight(0.0)
    dim._validate_weight(50.0)
    dim._validate_weight(100.0)

    # Invalid weights
    with pytest.raises(ValueError):
        dim._validate_weight(-5.0)

    with pytest.raises(ValueError):
        dim._validate_weight(150.0)

# Test get_impact_level logic
def test_impact_level_calculation():
    """Test all threshold boundaries for impact level."""
    class TestDimension(DimensionStrategy):
        # ... implement all required methods
        pass

    dim = TestDimension()

    # Test all threshold boundaries
    assert dim.get_impact_level(100.0) == "NONE"   # gap = 0, < 5
    assert dim.get_impact_level(96.0) == "NONE"    # gap = 4, < 5
    assert dim.get_impact_level(95.0) == "LOW"     # gap = 5, < 15
    assert dim.get_impact_level(92.0) == "LOW"     # gap = 8, < 15
    assert dim.get_impact_level(85.0) == "LOW"     # gap = 15, < 15 (boundary)
    assert dim.get_impact_level(84.0) == "MEDIUM"  # gap = 16, < 30
    assert dim.get_impact_level(75.0) == "MEDIUM"  # gap = 25, < 30
    assert dim.get_impact_level(70.0) == "MEDIUM"  # gap = 30, < 30 (boundary)
    assert dim.get_impact_level(69.0) == "HIGH"    # gap = 31, >= 30
    assert dim.get_impact_level(50.0) == "HIGH"    # gap = 50, >= 30
    assert dim.get_impact_level(0.0) == "HIGH"     # gap = 100, >= 30

# Test _calculate_gap static method
def test_calculate_gap():
    """Test gap calculation from perfect score."""
    assert DimensionStrategy._calculate_gap(100.0) == 0.0
    assert DimensionStrategy._calculate_gap(75.0) == 25.0
    assert DimensionStrategy._calculate_gap(0.0) == 100.0

# Test AST helper methods
def test_ast_helpers():
    """Test AST parsing and traversal methods."""
    class TestDimension(DimensionStrategy):
        # ... implement required methods
        pass

    dim = TestDimension()

    # Test parser lazy loading
    parser1 = dim._get_markdown_parser()
    parser2 = dim._get_markdown_parser()
    assert parser1 is parser2  # Should be singleton

    # Test AST parsing
    text = "# Heading\nParagraph text"
    ast = dim._parse_to_ast(text, cache_key='test')
    assert ast is not None

    # Test caching
    ast2 = dim._parse_to_ast(text, cache_key='test')
    assert ast is ast2  # Should return cached version

    # Test AST walking
    from marko.block import Heading
    headings = dim._walk_ast(ast, Heading)
    assert len(headings) > 0

    # Test text extraction
    text_content = dim._extract_text_from_node(headings[0])
    assert "Heading" in text_content

# Test backward compatibility - score() method
def test_score_method_backward_compatibility():
    """Verify score() method wraps calculate_score() correctly."""
    class TestDimension(DimensionStrategy):
        def calculate_score(self, metrics):
            return 85.0

        def get_tiers(self):
            return {
                'excellent': (90.0, 100.0),
                'good': (75.0, 89.9),
                'acceptable': (50.0, 74.9),
                'poor': (0.0, 49.9)
            }
        # ... other required methods

    dim = TestDimension()
    score_value, score_label = dim.score({'test': 'data'})

    assert score_value == 85.0
    assert score_label == "GOOD"  # 85.0 falls in 'good' tier

# Test analyze_detailed default implementation
def test_analyze_detailed_default():
    """Verify analyze_detailed returns empty list by default."""
    class TestDimension(DimensionStrategy):
        # ... implement required methods
        pass

    dim = TestDimension()
    result = dim.analyze_detailed(['line 1', 'line 2'])

    assert result == []
    assert isinstance(result, list)

# Test get_max_score deprecated method
def test_get_max_score_deprecated():
    """Verify get_max_score returns 100.0."""
    class TestDimension(DimensionStrategy):
        # ... implement required methods
        pass

    dim = TestDimension()
    assert dim.get_max_score() == 100.0

# Integration test - both bases coexist
def test_both_bases_coexist():
    """Verify old and new base classes can coexist."""
    from ai_pattern_analyzer.dimensions.base import DimensionAnalyzer
    from ai_pattern_analyzer.dimensions.base_strategy import DimensionStrategy

    # Both should be importable
    assert DimensionAnalyzer is not None
    assert DimensionStrategy is not None

    # Both should be ABCs
    from abc import ABC
    assert issubclass(DimensionAnalyzer, ABC)
    assert issubclass(DimensionStrategy, ABC)
```

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-03 | 1.0 | Initial story creation | Sarah (PO Agent) |
| 2025-11-03 | 2.0 | Enhanced with deep dive findings: AST helpers, backward compatibility, validation helpers, example implementation | Sarah (PO Agent) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
None - implementation completed without issues

### Completion Notes
Successfully implemented the enhanced DimensionStrategy base class with all required features:

1. **Core Implementation** - Created `dimensions/base_strategy.py` with:
   - DimensionTier enum (ADVANCED, CORE, SUPPORTING, STRUCTURAL)
   - DimensionStrategy ABC with all abstract properties and methods
   - Full type hints throughout (typing.Dict, List, Tuple, Any, Optional)
   - Comprehensive docstrings with usage examples

2. **Abstract Contract** - Implemented required abstract properties:
   - dimension_name: str (unique identifier)
   - weight: float (0-100 contribution weight)
   - tier: DimensionTier (classification tier)
   - description: str (human-readable description)

3. **Abstract Methods** - Implemented required abstract methods:
   - analyze(text, lines, **kwargs) -> Dict[str, Any]
   - calculate_score(metrics) -> float (0-100 scale)
   - get_recommendations(score, metrics) -> List[str]
   - get_tiers() -> Dict[str, Tuple[float, float]]

4. **Concrete Methods** - Implemented helper methods:
   - get_impact_level(score) -> str (gap-based impact calculation)
   - score(analysis_results) -> Tuple[float, str] (backward compatibility wrapper)
   - analyze_detailed(lines, html_comment_checker) -> List[Any] (optional override)

5. **AST Helpers** - Migrated from existing base.py:
   - _get_markdown_parser() - lazy-loaded singleton
   - _parse_to_ast(text, cache_key) - with caching support
   - _walk_ast(node, node_type) - recursive traversal
   - _extract_text_from_node(node) - text extraction

6. **Validation Helpers** - Implemented validation methods:
   - _validate_tier() - validates DimensionTier enum
   - _validate_score(score) - ensures 0-100 range
   - _validate_weight(weight) - ensures 0-100 range
   - _calculate_gap(score) - static method for gap calculation
   - _map_score_to_tier(score) - maps score to tier label

7. **Backward Compatibility** - Maintained compatibility with DimensionAnalyzer:
   - score() method wraps calculate_score() and adds tier label
   - get_max_score() returns 100.0 (deprecated but functional)
   - get_dimension_name() reads dimension_name property
   - Both base classes coexist without conflicts

8. **Module Updates** - Updated `dimensions/__init__.py` to export:
   - DimensionStrategy (new base class)
   - DimensionTier (enum)
   - DimensionAnalyzer (existing base, maintained for compatibility)

9. **Comprehensive Testing**:
   - **Unit Tests**: 53 tests in `test_base_strategy.py` - 97% coverage
     - Abstract enforcement (cannot instantiate base)
     - Abstract method/property enforcement
     - Tier validation (valid/invalid values)
     - Score validation (0-100 range)
     - Weight validation (0-100 range)
     - get_impact_level() threshold boundaries
     - _calculate_gap() static method
     - AST helper methods (parser, parsing, caching, walking, extraction)
     - Backward compatibility (score, get_max_score, get_dimension_name)
     - analyze_detailed() default implementation
     - _map_score_to_tier() with boundary testing

   - **Integration Tests**: 26 tests in `test_base_compatibility.py` - 97% coverage
     - Both bases can be imported without conflicts
     - Both are abstract base classes
     - Neither can be instantiated directly
     - No namespace conflicts
     - Legacy dimensions work correctly
     - New dimensions work correctly
     - Both can analyze same text independently
     - Multiple instances coexist without state sharing
     - New dimension fulfills legacy interface requirements
     - Migration path validated

   - **All Tests Passing**: 79/79 tests passed (100% success rate)

10. **Code Quality**:
    - Full type hints on all methods and properties
    - Comprehensive docstrings with examples
    - Follows existing code style and patterns
    - No breaking changes to existing dimensions
    - Ready for dimension migration in Story 1.4

### File List
**Source Files Created:**
- `dimensions/base_strategy.py` (new) - Enhanced dimension base class with DimensionStrategy ABC and DimensionTier enum

**Source Files Modified:**
- `dimensions/__init__.py` - Added exports for DimensionStrategy and DimensionTier

**Test Files Created:**
- `tests/unit/dimensions/test_base_strategy.py` (new) - 53 comprehensive unit tests
- `tests/integration/test_base_compatibility.py` (new) - 26 backward compatibility integration tests

**Test Files Modified:**
- None - all new test files

**Total Files Changed:** 4 files (1 created + 1 modified source, 2 created tests)

## QA Results

### Review Date: 2025-11-03

### Reviewed By: Quinn (Test Architect)

### Executive Summary

**Gate Status: PASS** - This is exceptional foundational work that establishes a robust, extensible base contract for the dimension architecture. All 8 acceptance criteria are fully met with outstanding test coverage (97%) and excellent code quality.

### Code Quality Assessment

**Overall Rating: Excellent (98/100)**

The implementation demonstrates exceptional quality across all dimensions:

**Strengths:**
- **Architecture**: Clear separation of abstract contract vs concrete helpers following Strategy, Template Method, and Adapter patterns
- **Type Safety**: Complete type hints throughout using proper generic types (Dict[str, Any], List[str], etc.)
- **Documentation**: Comprehensive docstrings with practical usage examples for every method and property
- **Validation**: Robust validation helpers with clear, actionable error messages
- **Performance**: AST caching optimization with lazy-loaded parser singleton
- **Backward Compatibility**: Thoughtful design allowing both base classes to coexist seamlessly during transition

**Code Organization:**
The implementation is logically structured with clear sections:
1. Abstract Properties (lines 124-173)
2. Abstract Methods (lines 179-285)
3. Concrete Methods (lines 291-349)
4. Backward Compatibility Methods (lines 355-397)
5. AST Helper Methods (lines 403-505)
6. Validation Helper Methods (lines 511-617)

### Refactoring Performed

No refactoring was necessary. The code quality is production-ready as-is.

### Compliance Check

- **Coding Standards**: ✓ No project-specific coding standards file found, but code follows Python best practices
- **Project Structure**: ✓ Files correctly placed in dimensions/ with proper __init__.py exports
- **Testing Strategy**: ✓ Exceeds standards with 97% coverage across unit and integration tests
- **All ACs Met**: ✓ All 8 acceptance criteria fully validated (see Requirements Traceability below)

### Requirements Traceability

Comprehensive mapping of acceptance criteria to validation:

**AC1: New DimensionStrategy ABC created in dimensions/base_strategy.py** ✓
- **Validated by**: test_cannot_instantiate_base(), test_abstract_method_enforcement()
- **Evidence**: Base class properly abstract, cannot be instantiated directly

**AC2: Must include all required abstract methods and properties** ✓
- **Validated by**: test_abstract_method_enforcement(), test_dimension_name_property(), test_weight_property(), test_tier_property(), test_description_property(), test_analyze_method(), test_calculate_score_method(), test_get_recommendations_method(), test_get_tiers_method()
- **Evidence**: All required properties (dimension_name, weight, tier, description) and methods (analyze, calculate_score, get_recommendations, get_tiers) implemented as abstract

**AC3: Full type hints on all methods** ✓
- **Validated by**: Code review of dimensions/base_strategy.py lines 13-15 (imports), all method signatures
- **Evidence**: Complete type annotations using proper generic types throughout (Dict[str, Any], List[str], Tuple[float, float], Optional[str])

**AC4: Comprehensive docstrings with examples** ✓
- **Validated by**: Code review of dimensions/base_strategy.py
- **Evidence**:
  - Module-level docstring (lines 1-11)
  - DimensionTier enum docstring with tier descriptions (lines 23-50)
  - DimensionStrategy class docstring with complete usage example (lines 52-112)
  - Every method has detailed docstring with args, returns, raises, and usage examples

**AC5: No breaking changes to existing dimensions yet** ✓
- **Validated by**: test_legacy_dimension_works(), test_both_dimensions_work_independently(), all 26 integration tests
- **Evidence**: Legacy dimensions using DimensionAnalyzer continue to work without modification

**AC6: Maintains backward compatibility with existing DimensionAnalyzer base class** ✓
- **Validated by**: test_both_bases_can_be_imported(), test_no_namespace_conflicts(), test_new_dimension_backward_compatible_score_method(), test_backward_compatible_score_signature(), test_both_dimensions_support_kwargs()
- **Evidence**: Both base classes coexist, new base supports legacy score() signature, no namespace conflicts

**AC7: Includes AST helper methods from existing base.py** ✓
- **Validated by**: test_get_markdown_parser(), test_parse_to_ast(), test_parse_to_ast_caching(), test_walk_ast_find_headings(), test_walk_ast_find_quotes(), test_walk_ast_find_code_blocks(), test_extract_text_from_node()
- **Evidence**: All 4 AST helper methods implemented (_get_markdown_parser, _parse_to_ast, _walk_ast, _extract_text_from_node) with caching support

**AC8: Provides validation helpers for tier, weight, and score values** ✓
- **Validated by**: test_tier_validation_with_valid_values(), test_tier_validation_with_invalid_value(), test_score_validation_valid_scores(), test_score_validation_invalid_negative(), test_score_validation_invalid_above_100(), test_weight_validation_valid_weights(), test_weight_validation_invalid_negative(), test_weight_validation_invalid_above_100()
- **Evidence**: Comprehensive validation methods (_validate_tier, _validate_score, _validate_weight) with proper range checking and error messages

### Test Architecture Assessment

**Test Coverage: 97% (Excellent)**

**Test Suite Composition:**
- **Unit Tests**: 53 tests in tests/unit/dimensions/test_base_strategy.py
- **Integration Tests**: 26 tests in tests/integration/test_base_compatibility.py
- **Total**: 79 tests, all passing

**Test Quality: Excellent**
- Comprehensive coverage of abstract enforcement
- Boundary value testing (e.g., score 0.0, 50.0, 100.0, 100.1)
- Error condition testing (negative values, out-of-range values)
- Cache behavior validation
- State isolation testing

**Test Organization:**
Well-structured with logical grouping:
- Abstract Base Class Tests
- Abstract Properties Tests
- Abstract Methods Tests
- Tier Validation Tests
- Score Validation Tests
- Weight Validation Tests
- get_impact_level() Tests
- AST Helper Methods Tests
- Backward Compatibility Tests
- Integration/Coexistence Tests

**Test Design Patterns:**
- Proper use of pytest fixtures for reusable test instances
- Parametric testing where appropriate (tier validation with all enum values)
- Clear test naming following pattern: test_{what}_{condition}
- Mock dimensions for testing abstract enforcement

### Security Review

**Status: PASS**

No security concerns identified. This is internal architecture code with no external exposure:
- No user input handling
- No file system operations beyond standard Python imports
- No network operations
- No authentication/authorization requirements
- Validation methods prevent invalid internal state

### Performance Considerations

**Status: PASS**

Performance optimizations implemented:
- **AST Parser**: Lazy-loaded singleton pattern (lines 415-417) prevents repeated parser instantiation
- **AST Caching**: Cache with configurable keys (lines 419-448) prevents re-parsing same markdown
- **Static Method**: _calculate_gap() as static (line 579) for efficiency when called frequently
- **Validation**: Early validation in calculate_score() prevents propagation of invalid values

No performance issues identified.

### Code Quality Observations

**One Minor Low-Severity Observation:**

**Location**: dimensions/base_strategy.py:498-499
**Issue**: Condition `isinstance(node.children, str)` may be unreachable
**Explanation**: The `children` attribute is typically a list in marko AST nodes, so this condition might never match
**Impact**: None - defensive programming that doesn't affect functionality
**Severity**: Low
**Recommendation**: Consider verifying this edge case with marko library documentation in future refactoring

### NFR Validation Summary

| NFR Category | Status | Notes |
|--------------|--------|-------|
| **Security** | PASS | No security concerns for internal architecture code |
| **Performance** | PASS | AST caching and lazy loading optimize markdown parsing |
| **Reliability** | PASS | Comprehensive validation with clear error messages |
| **Maintainability** | PASS | Excellent documentation, type hints, and code organization |

### Files Modified During Review

None - no code modifications were necessary. The implementation quality is production-ready.

### Improvements Checklist

All items already completed by development:

- [x] Complete abstract base class implementation with all required properties/methods
- [x] Full type hints throughout entire codebase
- [x] Comprehensive docstrings with usage examples
- [x] AST helper methods with caching optimization
- [x] Validation helpers for tier, weight, and score
- [x] Backward compatibility methods (score, get_max_score, get_dimension_name)
- [x] DimensionTier enum with clear documentation
- [x] Module exports in __init__.py
- [x] Unit tests with 97% coverage
- [x] Integration tests for backward compatibility
- [x] All 79 tests passing

### Future Recommendations

**For Story 1.4 (Dimension Migration):**

1. **Migration Guide**: Consider creating a developer migration guide documenting the steps to convert existing dimensions from DimensionAnalyzer to DimensionStrategy

2. **Edge Case Verification**: During migration, verify the _extract_text_from_node behavior with real marko AST nodes to confirm the line 498-499 edge case handling

3. **Example Dimension**: Consider using one dimension as a reference implementation showing the complete migration pattern

4. **Weight Validation**: Story 1.3 (WeightMediator) should validate that all dimension weights sum to exactly 100.0 (±0.1% tolerance)

### Gate Status

**Gate: PASS** → docs/qa/gates/1.1-enhanced-dimension-base.yml

**Quality Score: 98/100**

### Recommended Status

✓ **Ready for Done**

This story fully meets all acceptance criteria with exceptional quality. The foundation is solid for the upcoming dimension migration work (Stories 1.2-1.4). No changes required.

### Summary

This is exemplary foundational work that sets a high standard for the project. The implementation demonstrates:

- Complete fulfillment of all 8 acceptance criteria
- Outstanding test coverage (97%) with comprehensive validation
- Excellent architectural design following established patterns
- Production-ready code quality with full type safety and documentation
- Thoughtful backward compatibility enabling smooth transition
- Strong foundation for dimension self-registration architecture

The code is ready for production use and provides an excellent model for the dimension migrations to follow in Story 1.4.
