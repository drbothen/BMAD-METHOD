# Story 5.3: Security Reviewer Agent Usage Guide

## Status

Draft

## Story

**As a** security reviewer,
**I want** comprehensive usage documentation for the Security Reviewer agent,
**so that** I can effectively review enrichments using systematic quality evaluation and provide constructive feedback.

## Acceptance Criteria

1. Documentation covers Security Reviewer agent activation and persona overview
2. Complete command reference for all 5 commands (*help, *review-enrichment, *fact-check, *detect-bias, *generate-report, *exit)
3. Detailed explanation of 8 quality dimensions with scoring methodology
4. Best practices for conducting blameless, constructive peer reviews
5. Understanding review outputs and feedback delivery
6. Examples of excellent, good, and needs-improvement enrichments with review feedback

## Tasks / Subtasks

- [ ] Create Security Reviewer agent usage guide (AC: 1)
  - [ ] Create `docs/user-guide/security-reviewer-agent.md`
  - [ ] Document agent activation: `/bmad-1898:agents:security-reviewer`
  - [ ] Explain agent persona: Riley - Security Review Specialist
  - [ ] Describe review philosophy: Blameless, constructive, educational
- [ ] Document agent commands (AC: 2)
  - [ ] `*help` - Display available commands
  - [ ] `*review-enrichment {ticket-id}` - Complete 7-stage review workflow
  - [ ] `*fact-check {ticket-id}` - Verify factual claims using Perplexity
  - [ ] `*detect-bias {ticket-id}` - Run cognitive bias detection
  - [ ] `*generate-report {ticket-id}` - Create review report from findings
  - [ ] `*exit` - Exit agent mode
- [ ] Document 8 quality dimensions (AC: 3)
  - [ ] Dimension 1: Technical Accuracy (25% weight) - CVSS/EPSS/KEV correctness, patch versions, exploit status
  - [ ] Dimension 2: Completeness (20% weight) - All 12 sections populated, no critical gaps
  - [ ] Dimension 3: Actionability (15% weight) - Clear remediation steps, testable guidance
  - [ ] Dimension 4: Contextualization (15% weight) - Business impact, ACR, exposure assessment
  - [ ] Dimension 5: Documentation Quality (10% weight) - Formatting, clarity, organization
  - [ ] Dimension 6: Attack Mapping Validation (5% weight) - ATT&CK tactics/techniques accuracy
  - [ ] Dimension 7: Cognitive Bias (5% weight) - Detect 5 bias types (confirmation, anchoring, availability, overconfidence, recency)
  - [ ] Dimension 8: Source Citation (5% weight) - Authoritative sources cited correctly
  - [ ] Overall score calculation: Weighted average ‚Üí 0-100 classification
- [ ] Create blameless review best practices (AC: 4)
  - [ ] Blameless culture principles
  - [ ] Constructive feedback techniques (strengths first, gaps second)
  - [ ] Educational approach (link gaps to learning resources)
  - [ ] Avoiding blame language (what vs. who)
  - [ ] Growth-oriented framing
  - [ ] Conversation starters for collaborative learning
- [ ] Document review outputs (AC: 5)
  - [ ] Review report structure (12 sections)
  - [ ] Quality dimension scores table
  - [ ] Gap categorization (Critical/Significant/Minor)
  - [ ] Recommendations and next steps
  - [ ] JIRA comment posting
  - [ ] Local review file location
- [ ] Create example reviews (AC: 6)
  - [ ] Example 1: Excellent enrichment (95/100) - strengths highlighted, minor improvements suggested
  - [ ] Example 2: Good enrichment (82/100) - solid work with some significant gaps
  - [ ] Example 3: Needs improvement (68/100) - constructive feedback for major gaps

## Dev Notes

### Epic Context

**Parent Epic:** Epic 5: User Documentation & Usage Guide
**This Story (5.3) Purpose:** Enable reviewers to conduct high-quality, systematic peer reviews using the 8-dimension framework with blameless culture principles.

**Dependencies:**
- Story 5.1: Installation & Initial Setup Guide
- Story 5.2: Security Analyst Agent Usage Guide (reviewers must understand enrichment process)
- Story 5.5: Security Analysis Review Workflow Deep Dive (complementary technical reference)

### Source Agent Definition

**Agent File:** `expansion-packs/bmad-1898-engineering/agents/security-reviewer.md`

**Agent Metadata:**
- Name: Riley
- ID: security-reviewer
- Title: Security Review Specialist
- Icon: üîç
- When to Use: Reviewing analyst enrichments, quality assurance, bias detection, constructive feedback

**Agent Persona:**
- Role: Senior security analyst performing peer review
- Style: Constructive, educational, thorough, respectful
- Identity: Quality mentor fostering continuous improvement
- Focus: Blameless review with growth-oriented feedback
- Core Principles:
  - Blameless culture (no blame/criticism, only improvement opportunities)
  - Constructive feedback (strengths before gaps)
  - Educational approach (link gaps to resources)
  - Systematic review (8-dimension checklists)
  - Bias awareness (detect without judgment)
  - Actionable recommendations (specific next steps)

### 8 Quality Dimensions Framework

**Dimension 1: Technical Accuracy (25% weight)**
- **Purpose:** Verify factual correctness of vulnerability data
- **Checklist Items:**
  - CVSS score matches NVD or vendor advisory
  - CVSS vector string is valid and accurate
  - EPSS score current (within 7 days)
  - KEV status correct per CISA catalog
  - Affected versions accurate per vendor advisory
  - Patched versions correct and specific
  - Exploit status matches threat intelligence
  - ATT&CK T-numbers valid
- **Scoring:** Pass/Fail per item ‚Üí percentage
- **Critical Issues:** Incorrect CVSS, wrong patch version, KEV status error

**Dimension 2: Completeness (20% weight)**
- **Purpose:** Ensure all required enrichment sections present
- **Checklist Items:**
  - All 12 template sections populated
  - Executive summary present and concise
  - Vulnerability details complete
  - Remediation guidance provided
  - Business impact assessment included
  - Threat intelligence researched
  - Sources cited for all claims
- **Scoring:** Section count / 12 sections ‚Üí percentage
- **Critical Issues:** Missing remediation guidance, missing priority assessment

**Dimension 3: Actionability (15% weight)**
- **Purpose:** Ensure remediation guidance is implementable
- **Checklist Items:**
  - Remediation steps are clear and specific
  - Patch version or workaround provided
  - Verification steps included
  - Compensating controls listed (if applicable)
  - Guidance is appropriate for target audience (DevOps/SysAdmin)
  - Estimated remediation effort noted
- **Scoring:** Actionable guidance present ‚Üí percentage
- **Significant Issues:** Vague guidance ("update software"), no verification steps

**Dimension 4: Contextualization (15% weight)**
- **Purpose:** Ensure business context informs risk assessment
- **Checklist Items:**
  - Asset Criticality Rating assessed
  - System Exposure classified
  - Business processes affected identified
  - Business impact clearly articulated
  - Priority rationale references business context
  - Stakeholders identified
- **Scoring:** Context elements present ‚Üí percentage
- **Significant Issues:** Missing ACR, missing business impact

**Dimension 5: Documentation Quality (10% weight)**
- **Purpose:** Ensure enrichment is well-formatted and clear
- **Checklist Items:**
  - Markdown formatting correct
  - Spelling and grammar professional
  - Section headings consistent
  - Lists and tables formatted properly
  - Clarity (no ambiguous language)
  - Organization logical
- **Scoring:** Quality elements present ‚Üí percentage
- **Minor Issues:** Formatting inconsistencies, typos

**Dimension 6: Attack Mapping Validation (5% weight)**
- **Purpose:** Verify MITRE ATT&CK mapping accuracy
- **Checklist Items:**
  - Tactics are valid ATT&CK tactics
  - Techniques have valid T-numbers
  - Mapping is appropriate for vulnerability type
  - Detection implications included
  - Defense recommendations aligned with mapping
- **Scoring:** Mapping elements correct ‚Üí percentage
- **Significant Issues:** Invalid T-numbers, incorrect tactic mapping

**Dimension 7: Cognitive Bias (5% weight)**
- **Purpose:** Detect cognitive biases in analysis
- **5 Bias Types:**
  1. **Confirmation Bias:** Seeking only information that confirms initial assessment
  2. **Anchoring Bias:** Over-relying on first piece of information (initial CVSS)
  3. **Availability Bias:** Overweighting recent/memorable incidents
  4. **Overconfidence Bias:** Expressing certainty without sufficient evidence
  5. **Recency Bias:** Focusing on most recent threat intelligence, ignoring historical context
- **Detection:** Review enrichment for bias patterns using checklist
- **Scoring:** Bias-free analysis ‚Üí 100%, biases detected ‚Üí lower score
- **Debiasing Recommendations:** Suggest alternative perspectives, additional research

**Dimension 8: Source Citation (5% weight)**
- **Purpose:** Ensure authoritative sources cited
- **Checklist Items:**
  - All factual claims have sources
  - Sources are authoritative (NIST NVD, CISA, vendor advisories, FIRST)
  - URLs included for all sources
  - Sources are current (within reasonable timeframe)
  - No reliance on non-authoritative sources (forums, blogs) without corroboration
- **Scoring:** Citation elements present ‚Üí percentage
- **Significant Issues:** Missing sources for CVSS/EPSS/KEV, relying on unverified sources

**Overall Quality Score:**
```
Quality Score = (
  Technical_Accuracy √ó 0.25 +
  Completeness √ó 0.20 +
  Actionability √ó 0.15 +
  Contextualization √ó 0.15 +
  Documentation_Quality √ó 0.10 +
  Attack_Mapping √ó 0.05 +
  Cognitive_Bias √ó 0.05 +
  Source_Citation √ó 0.05
)
```

**Classification:**
- 90-100: Excellent
- 75-89: Good
- 60-74: Needs Improvement
- < 60: Inadequate

### Blameless Review Best Practices

**Principle 1: Blameless Culture**
- Reviews focus on work quality, not analyst performance
- No blame language: Avoid "you made an error" ‚Üí Use "this section could be improved"
- Assume good intent: Analysts did their best with available information
- Normalize mistakes: Errors are learning opportunities, not failures
- Systemic focus: If multiple analysts make same mistake, it's a process issue, not individual fault

**Principle 2: Constructive Feedback**
- **Strengths First:** Always start review with "What Went Well" section
- Acknowledge good work explicitly before identifying gaps
- Balance: Aim for 2:1 ratio of positive to improvement feedback (for Good/Excellent enrichments)
- Specific praise: "Excellent remediation guidance with clear step-by-step patch instructions"
- Growth framing: "To further improve..." rather than "You failed to..."

**Principle 3: Educational Approach**
- Every gap includes learning resource link
- Conversation starters encourage collaborative learning
- Recommendations explain "why" not just "what"
- Link to knowledge base articles, checklists, training materials
- Mentor tone: Helping colleague grow, not judging performance

**Principle 4: Actionable Recommendations**
- Every gap has specific next step
- Prioritized: Critical ‚Üí Significant ‚Üí Minor
- Clear remediation: "Add CISA KEV check using [URL]" not "Check KEV"
- Estimated effort: "5-minute fix" vs. "requires re-research"

**Principle 5: Avoid Blame Language**
- ‚ùå "You failed to check KEV status"
- ‚úÖ "KEV status verification is missing - recommend checking CISA catalog"
- ‚ùå "This is wrong"
- ‚úÖ "CVSS score differs from NVD - verify using [NVD link]"
- ‚ùå "Poor quality enrichment"
- ‚úÖ "Some gaps identified - see recommendations below to address"

### Review Output Structure

**Review Report (12 Sections):**
1. **Review Metadata:** Ticket ID, CVE, analyst, reviewer, date, quality score
2. **Executive Summary:** 2-3 sentences, constructive tone, overall assessment
3. **Strengths & What Went Well:** Acknowledge positive work (always include)
4. **Quality Dimension Scores:** Table with 8 dimensions, scores, assessment
5. **Critical Issues:** (If any) High-impact gaps requiring immediate attention
6. **Significant Gaps:** Important findings with constructive language
7. **Minor Improvements:** Optional enhancements, lower priority
8. **Cognitive Bias Assessment:** Detected biases with debiasing recommendations
9. **Fact Verification Results:** (If performed) Discrepancies documented
10. **Recommendations & Learning Resources:** Prioritized next steps with links
11. **Conversation Starters:** Educational questions to foster discussion
12. **Next Steps:** Clear action items for analyst

**Example Quality Dimension Scores Table:**
| Dimension | Score | Weight | Weighted | Assessment |
|-----------|-------|--------|----------|------------|
| Technical Accuracy | 95% | 25% | 23.75 | Excellent - all metrics verified |
| Completeness | 100% | 20% | 20.00 | Excellent - all sections present |
| Actionability | 85% | 15% | 12.75 | Good - remediation clear, verification could be more detailed |
| Contextualization | 80% | 15% | 12.00 | Good - business impact present, processes could be more specific |
| Documentation Quality | 90% | 10% | 9.00 | Excellent - well-formatted and clear |
| Attack Mapping | 100% | 5% | 5.00 | Excellent - accurate T-numbers and mapping |
| Cognitive Bias | 90% | 5% | 4.50 | Good - minor confirmation bias detected |
| Source Citation | 95% | 5% | 4.75 | Excellent - all claims sourced |
| **TOTAL** | | | **91.75** | **Excellent** |

### Example Reviews

**Example 1: Excellent Enrichment (95/100)**

**Scenario:** CVE-2024-5678 PostgreSQL Privilege Escalation
**Analyst:** Jordan
**Reviewer:** Riley

**Executive Summary:**
Outstanding enrichment with comprehensive research, accurate technical details, and highly actionable remediation guidance. Jordan demonstrated excellent use of authoritative sources and thorough business context assessment. Minor suggestion for additional verification steps.

**Strengths & What Went Well:**
- Exceptional technical accuracy - all metrics verified against multiple authoritative sources
- Complete 12-section enrichment with detailed analysis in each section
- Remediation guidance is highly actionable with specific commands and version numbers
- Excellent business impact assessment clearly articulating risk to database operations
- Proper MITRE ATT&CK mapping with relevant detection implications
- All claims properly cited with authoritative sources

**Quality Dimension Scores:** (see table above - 91.75/100)

**Significant Gaps:** None identified

**Minor Improvements:**
1. **Verification Steps:** Consider adding specific re-test procedures post-patch (e.g., "Verify with `SELECT version();` shows 15.3.2+")
   - Resource: docs/data/verification-best-practices.md
   - Effort: 2 minutes

**Cognitive Bias Assessment:**
Minor confirmation bias detected in Threat Intelligence section (focused on exploitation evidence, could balance with non-exploitation indicators). This is very minor and doesn't impact overall quality.

**Recommendations:**
1. Add specific verification command for post-patch validation

**Conversation Starters:**
- What verification steps do you typically use for database patches?
- How do you balance thoroughness with time constraints on lower-priority vulnerabilities?

**Next Steps:**
- Optional: Enhance verification section (2-minute update)
- Excellent work - no critical revisions needed
- Approved for remediation

---

**Example 2: Good Enrichment (82/100)**

**Scenario:** CVE-2024-9012 Nginx Path Traversal
**Analyst:** Alex
**Reviewer:** Riley

**Executive Summary:**
Solid enrichment with accurate technical analysis and good remediation guidance. Some gaps in business context detail and source citations could be improved. Overall assessment: Good - meets quality standards with room for enhancement.

**Strengths & What Went Well:**
- Accurate CVSS and EPSS scores verified against NVD and FIRST
- Clear remediation guidance with specific Nginx version to patch
- All 12 template sections populated
- Good ATT&CK mapping with appropriate tactics and techniques
- Well-organized and professionally formatted

**Quality Dimension Scores:**
| Dimension | Score | Assessment |
|-----------|-------|------------|
| Technical Accuracy | 90% | Excellent |
| Completeness | 100% | Excellent |
| Actionability | 80% | Good - remediation clear, verification missing |
| Contextualization | 65% | Needs Improvement - business impact vague |
| Documentation Quality | 95% | Excellent |
| Attack Mapping | 85% | Good |
| Cognitive Bias | 85% | Good |
| Source Citation | 70% | Needs Improvement - some missing sources |
| **TOTAL** | **82.00** | **Good** |

**Significant Gaps:**
1. **Business Impact Vague:** "Affects web services" is too general
   - Recommendation: Specify which business processes (e.g., "Customer portal login, API authentication")
   - Resource: docs/data/business-impact-assessment-guide.md
   - Effort: 5 minutes

2. **Missing Sources:** EPSS score cited without source URL
   - Recommendation: Add FIRST EPSS source: https://www.first.org/epss/
   - Resource: docs/checklists/source-citation-checklist.md
   - Effort: 2 minutes

3. **Verification Steps Missing:** No post-patch verification guidance
   - Recommendation: Add "Test with curl command targeting path traversal pattern to verify patch"
   - Effort: 3 minutes

**Minor Improvements:**
4. **Related CVEs:** Section states "None" - consider checking for Nginx CVEs from same month
   - Resource: NVD search by product and date range
   - Effort: 5 minutes (optional)

**Recommendations (Prioritized):**
1. **HIGH:** Enhance business impact section with specific processes (5 min)
2. **HIGH:** Add EPSS source citation (2 min)
3. **MEDIUM:** Add verification steps (3 min)
4. **LOW:** Research related CVEs (optional 5 min)

**Conversation Starters:**
- How do you typically gather business impact information for web servers?
- What verification approaches work best for path traversal vulnerabilities in your experience?

**Next Steps:**
1. Address 3 HIGH/MEDIUM gaps (estimated 10 minutes total)
2. Re-submit for verification review (optional - trust but verify)
3. Approved for remediation with recommended enhancements

---

**Example 3: Needs Improvement (68/100)**

**Scenario:** CVE-2024-3456 Jenkins Authentication Bypass
**Analyst:** Sam
**Reviewer:** Riley

**Executive Summary:**
Enrichment demonstrates good research effort with some technical accuracy. However, several critical gaps in remediation guidance and business context need to be addressed before proceeding to remediation. This review is intended to help improve enrichment quality - see detailed recommendations below.

**Strengths & What Went Well:**
- CVE research performed using Perplexity
- Template structure followed with all 12 sections present
- CVSS score verified against NVD
- Clear formatting and organization

**Quality Dimension Scores:**
| Dimension | Score | Assessment |
|-----------|-------|------------|
| Technical Accuracy | 75% | Good - some KEV status discrepancy |
| Completeness | 85% | Good - some sections minimal |
| Actionability | 45% | Inadequate - remediation too vague |
| Contextualization | 50% | Inadequate - missing ACR and impact |
| Documentation Quality | 90% | Excellent |
| Attack Mapping | 60% | Needs Improvement - T-number invalid |
| Cognitive Bias | 70% | Needs Improvement - confirmation bias detected |
| Source Citation | 55% | Inadequate - missing key sources |
| **TOTAL** | **68.00** | **Needs Improvement** |

**Critical Issues:**
1. **Vague Remediation Guidance:** "Update Jenkins" - no specific version provided
   - **Impact:** DevOps cannot act without specific patch version
   - **Recommendation:** Research vendor advisory for specific patched version (e.g., "Update to Jenkins 2.440.2 or later")
   - **Resource:** Jenkins Security Advisory: https://www.jenkins.io/security/advisories/
   - **Effort:** 10 minutes
   - **Next Step:** Re-research CVE-2024-3456 patch details

2. **Missing Business Context:** ACR and System Exposure fields not populated
   - **Impact:** Priority assessment may be incorrect
   - **Recommendation:** Consult asset inventory for Jenkins CI/CD server criticality, confirm network exposure
   - **Resource:** docs/data/business-context-assessment-guide.md
   - **Effort:** 5-10 minutes
   - **Next Step:** Contact asset owner or check CMDB

**Significant Gaps:**
3. **KEV Status Discrepancy:** Enrichment states "KEV: No" but CISA catalog shows CVE-2024-3456 added 2024-10-15
   - **Recommendation:** Re-check CISA KEV catalog: https://www.cisa.gov/known-exploited-vulnerabilities-catalog
   - **Effort:** 3 minutes

4. **Invalid ATT&CK Technique:** T-9999 is not a valid MITRE ATT&CK technique number
   - **Recommendation:** Authentication bypass typically maps to T1078 (Valid Accounts) or T1110 (Brute Force)
   - **Resource:** MITRE ATT&CK Matrix: https://attack.mitre.org/
   - **Effort:** 5 minutes

5. **Missing EPSS Source:** EPSS score cited without verification source
   - **Recommendation:** Add FIRST EPSS source URL
   - **Effort:** 2 minutes

**Cognitive Bias Assessment:**
**Confirmation Bias Detected:** Threat Intelligence section focuses only on absence of exploitation evidence, without balanced research into exploitation likelihood. Recommendation: Research both exploitation and non-exploitation indicators for balanced analysis.

**Recommendations (Prioritized):**
**CRITICAL (Must Fix Before Remediation):**
1. Research specific Jenkins patch version (10 min)
2. Gather business context (ACR, Exposure) (5-10 min)

**HIGH (Should Fix):**
3. Verify KEV status against CISA catalog (3 min)
4. Correct ATT&CK T-number (5 min)
5. Add EPSS source citation (2 min)

**MEDIUM (Recommended):**
6. Balance threat intelligence analysis to avoid confirmation bias

**Total Estimated Effort:** 25-35 minutes for critical and high-priority fixes

**Learning Resources:**
- Remediation Guidance Best Practices: docs/data/remediation-guidance-best-practices.md
- Business Context Assessment: docs/data/business-context-assessment-guide.md
- MITRE ATT&CK Mapping Guide: docs/data/mitre-attack-mapping-guide.md
- Source Citation Standards: docs/checklists/source-citation-checklist.md

**Conversation Starters:**
- What challenges did you encounter researching this CVE?
- How can I help you improve the efficiency of your remediation research?
- Would a vendor advisory checklist be helpful for future enrichments?

**Next Steps:**
1. Address 2 CRITICAL issues (15-20 minutes)
2. Address 3 HIGH issues (13 minutes)
3. Re-submit enrichment for review
4. Schedule optional 15-minute pairing session to discuss process improvements

**Status:** Return to analyst for remediation of critical and high-priority gaps

## Testing

**Test Location:** `expansion-packs/bmad-1898-engineering/tests/documentation/`

**Testing Standards:**

**Documentation Accuracy Testing:**
- All 8 quality dimensions accurately described and match checklist implementations
- Scoring methodology matches actual weighted calculation
- Example reviews are realistic and demonstrate blameless principles
- All command syntax verified against agent implementation

**Usability Testing:**
- New reviewers can conduct first review using only this guide
- 8-dimension framework is clear and actionable
- Blameless principles are understood and applied
- Example reviews demonstrate appropriate tone and feedback quality

**Test Cases:**

1. **TC-DOC-020: Quality Dimensions Accuracy**
   - Objective: Verify all 8 dimensions match actual checklists
   - Test: Compare documentation to checklist files for each dimension
   - Expected: 100% alignment between docs and implementation

2. **TC-DOC-021: Scoring Calculation Accuracy**
   - Objective: Verify weighted score calculation is correct
   - Test: Use example enrichment, calculate score manually vs. agent calculation
   - Expected: Scores match within 1 point

3. **TC-DOC-022: Blameless Review Tone**
   - Objective: Verify example reviews demonstrate blameless principles
   - Test: Review example feedback for blame language, constructive tone
   - Expected: All examples use blameless language, strengths-first approach

4. **TC-DOC-023: New Reviewer Effectiveness**
   - Objective: Verify new reviewers can conduct quality reviews using guide
   - Test: New reviewer conducts first review using only this documentation
   - Expected: Review includes all 8 dimensions, blameless tone, actionable feedback

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2025-11-08 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

_(To be populated during implementation)_

## QA Results

_(To be populated during QA review)_
