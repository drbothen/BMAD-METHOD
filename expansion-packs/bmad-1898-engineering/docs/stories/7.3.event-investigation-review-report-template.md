# Story 7.3: Event Investigation Review Report Template

## Status

Pending

## Story

**As a** Security Reviewer agent,
**I want** a structured template for event investigation review reports,
**so that** I can provide consistent, constructive feedback on investigation quality with disposition agreement/disagreement tracking.

## Acceptance Criteria

1. YAML template created following BMAD template format standards
2. Template includes all sections from Epic 7 PRD FR-3 (metadata, executive summary, strengths, quality scores, issues, disposition assessment, recommendations)
3. Template supports disposition agreement/disagreement tracking with reviewer reasoning
4. Template maintains blameless, constructive tone through language patterns
5. Template compatible with create-doc.md task for report generation
6. Template includes 7 quality dimension scores with weighted overall score
7. Template formats correctly for JIRA comment posting

## Tasks / Subtasks

- [ ] Create security-event-investigation-review-report-tmpl.yaml (AC: 1, 2, 3, 4, 5, 6, 7)
  - [ ] Create `expansion-packs/bmad-1898-engineering/templates/security-event-investigation-review-report-tmpl.yaml`
  - [ ] Follow BMAD template format specification
  - [ ] Add template metadata (name, version, description)
  - [ ] Define all sections with variable substitution fields

- [ ] Implement review metadata section (AC: 2)
  - [ ] ticket_id, alert_id, alert_type (ICS/IDS/SIEM)
  - [ ] investigation_timestamp, analyst_name, reviewer_name, review_date

- [ ] Implement executive summary section (AC: 2, 3, 6)
  - [ ] quality_classification (Excellent|Good|Needs Improvement|Inadequate)
  - [ ] overall_score (weighted score from 7 dimensions)
  - [ ] disposition (TP|FP|BTP)
  - [ ] disposition_agreement (Agree|Disagree|Uncertain)
  - [ ] summary (2-3 sentence overview)

- [ ] Implement strengths section (AC: 4)
  - [ ] Acknowledge 3-5 positive aspects (blameless principle: strengths first)
  - [ ] Use constructive language patterns
  - [ ] Specific examples of what was done well

- [ ] Implement quality_scores section (AC: 6)
  - [ ] investigation_completeness (score + weight 25%)
  - [ ] technical_accuracy (score + weight 20%)
  - [ ] disposition_reasoning (score + weight 20%)
  - [ ] contextualization (score + weight 15%)
  - [ ] methodology (score + weight 10%)
  - [ ] documentation_quality (score + weight 5%)
  - [ ] cognitive_bias (score + weight 5%)
  - [ ] overall_score (calculated weighted average)

- [ ] Implement issues sections (AC: 2, 4)
  - [ ] critical_issues (severity: Critical, requires immediate correction)
  - [ ] significant_gaps (important improvements needed)
  - [ ] minor_improvements (optional enhancements)
  - [ ] Each issue includes: location, description, impact, recommendation, learning_resource
  - [ ] Use constructive language patterns (avoid blame)

- [ ] Implement disposition_assessment section (AC: 3)
  - [ ] analyst_disposition (TP|FP|BTP)
  - [ ] reviewer_disposition (TP|FP|BTP)
  - [ ] agreement (yes|no)
  - [ ] reasoning (if disagreement, explain with evidence)
  - [ ] This section is unique to event reviews (not in CVE review template)

- [ ] Implement cognitive_bias_assessment section (AC: 2)
  - [ ] detected_biases (list of identified biases)
  - [ ] bias_impact (how biases affected investigation)
  - [ ] debiasing_strategies (recommendations to counteract)
  - [ ] Include automation bias specific to event investigations

- [ ] Implement recommendations section (AC: 2, 4)
  - [ ] priority_actions (critical fixes, significant improvements, minor suggestions)
  - [ ] investigation_improvements (methodology enhancements, data sources, tools)
  - [ ] Use constructive language patterns

- [ ] Implement learning_resources section (AC: 2, 4)
  - [ ] List of resources with title, URL, topic
  - [ ] Link to event investigation best practices
  - [ ] Educational focus (growth mindset)

- [ ] Implement next_steps section (AC: 2)
  - [ ] Based on findings: Critical → Needs Revision, Significant → Review Recommended, Minor → Optional, Excellent → Approved
  - [ ] Clear guidance on expected actions

- [ ] Add LLM instructions for blameless tone (AC: 4)
  - [ ] Avoid blame patterns: "You missed", "This is wrong", "You failed to"
  - [ ] Use constructive patterns: "Consider adding", "This could benefit from", "A helpful addition would be"
  - [ ] Use "we" language, not "you" language
  - [ ] Frame gaps as learning opportunities

- [ ] Validate JIRA comment formatting (AC: 7)
  - [ ] Markdown-compatible formatting
  - [ ] Sections render correctly in JIRA comments
  - [ ] No formatting issues with special characters
  - [ ] Compatible with post-enrichment-comment.md posting logic

- [ ] Validate create-doc.md compatibility (AC: 5)
  - [ ] Template follows BMAD variable substitution syntax
  - [ ] All variables have clear names
  - [ ] Template includes section instructions for LLM generation
  - [ ] Testable with create-doc.md task

## Dev Notes

### Epic Context

**Epic 7: Security Event Investigation Review Capability**

This story creates the review report template that the Security Reviewer agent (Story 7.4) will use when providing feedback on event investigations. This template parallels the existing `security-review-report-tmpl.yaml` (used for CVE reviews) but is tailored to event investigation review needs.

**Key Differences from CVE Review Template:**
1. **Disposition Assessment Section:** Event reviews track whether reviewer agrees with analyst's disposition (TP/FP/BTP) - critical for event investigations
2. **7 Quality Dimensions vs. 8:** Different weighting and focus areas
3. **Automation Bias:** Added to cognitive bias assessment (specific to event investigations)
4. **Investigation Methodology:** Replaces "Actionability" and "Attack Mapping" dimensions

**Related Stories:**
- Story 7.2: Event Investigation Quality Checklists (provides scores for quality_scores section)
- **Story 7.3: Event Investigation Review Report Template** (This story)
- Story 7.4: Security Reviewer Auto-Detection and Event Review (uses this template)

### Relevant Source Tree

```
expansion-packs/bmad-1898-engineering/
├── templates/
│   ├── security-enrichment-tmpl.yaml              (Existing - CVE enrichment)
│   ├── security-review-report-tmpl.yaml           (Existing - CVE review reports)
│   ├── event-investigation-tmpl.yaml              (Story 7.1 - event investigation documents)
│   └── security-event-investigation-review-report-tmpl.yaml (TO BE CREATED - this story)
├── tasks/
│   ├── create-doc.md                              (Existing - uses templates)
│   └── review-security-enrichment.md              (Existing - will extend in Story 7.5)
├── agents/
│   └── security-reviewer.md                       (Existing - will use this template in Story 7.4)
└── docs/
    ├── prd/
    │   └── epic-7-security-event-investigation-review.md (FR-3 defines template structure)
    └── stories/
        └── 7.3.event-investigation-review-report-template.md (This file)
```

**File to Create:**
- `expansion-packs/bmad-1898-engineering/templates/security-event-investigation-review-report-tmpl.yaml`

### Template Structure (from Epic 7 PRD FR-3)

The template must include these sections as specified in the PRD:

1. **review_metadata:** Ticket, alert, analyst, reviewer identification
2. **executive_summary:** Quality classification, overall score, disposition agreement, summary
3. **strengths:** 3-5 positive aspects acknowledged
4. **quality_scores:** 7 dimension scores + weights + overall score
5. **critical_issues:** Issues requiring immediate correction
6. **significant_gaps:** Important improvements needed
7. **minor_improvements:** Optional enhancements
8. **disposition_assessment:** Analyst vs. reviewer disposition comparison (UNIQUE to event reviews)
9. **cognitive_bias_assessment:** Detected biases, impact, debiasing strategies
10. **recommendations:** Priority actions and investigation improvements
11. **learning_resources:** Educational materials with links
12. **next_steps:** Clear guidance based on review findings

### Disposition Agreement Tracking

This is the key differentiator between CVE review and event investigation review templates.

**Disposition Assessment Section Structure:**
```yaml
disposition_assessment:
  analyst_disposition: '{TP|FP|BTP}'
  reviewer_disposition: '{TP|FP|BTP}'
  agreement: '{yes|no}'
  reasoning: |
    {If disagreement, explain reasoning for alternate disposition
    with specific evidence and logic. If agreement, briefly confirm
    why the disposition is correct.}
```

**Example - Agreement:**
```yaml
disposition_assessment:
  analyst_disposition: 'False Positive'
  reviewer_disposition: 'False Positive'
  agreement: 'yes'
  reasoning: |
    Reviewer agrees with False Positive disposition. The evidence clearly demonstrates:
    - Source IP is authorized jump server (Asset DB verified)
    - SSH connection pattern matches scheduled backup (6 months history confirmed)
    - SSH keys match authorized IT Ops keys (key fingerprint validated)
    - No concurrent suspicious activity detected

    The analyst's conclusion is well-supported and logical.
```

**Example - Disagreement:**
```yaml
disposition_assessment:
  analyst_disposition: 'False Positive'
  reviewer_disposition: 'True Positive'
  agreement: 'no'
  reasoning: |
    Reviewer disagrees with False Positive disposition and believes this is True Positive (Confidence: Medium).

    Evidence supporting True Positive:
    - SSH connection uses non-standard port 2222 (not documented in jump server config)
    - Connection timing does not match scheduled backup window (occurred 14:30 UTC, backups run 02:00 UTC)
    - SSH key fingerprint does not match any authorized keys in IT Ops registry
    - Concurrent failed login attempts on other systems from same source IP (10 failed attempts in past hour)

    The analyst's investigation did not verify the SSH key fingerprint against the key registry
    or check for concurrent suspicious activity. The timing discrepancy and non-standard port
    suggest this may be unauthorized access rather than scheduled maintenance.

    Recommendation: Re-investigate with focus on SSH key validation and timeline correlation.
    Escalate to incident response team for containment assessment.
```

### Blameless Language Patterns

**Template Instructions Must Include:**

```yaml
instructions: |
  When generating this review report:

  BLAMELESS LANGUAGE REQUIREMENTS:
  - Always acknowledge strengths BEFORE identifying gaps
  - Use "we" language (collaborative), not "you" language (accusatory)
  - Frame gaps as "opportunities for improvement" not "failures" or "errors"
  - Provide specific, actionable recommendations for every gap
  - Link gaps to learning resources (educational approach)
  - Maintain constructive, respectful tone throughout

  AVOID THESE PATTERNS:
  - "You missed..." → USE: "An opportunity to strengthen this analysis would be adding..."
  - "This is wrong..." → USE: "Consider revising this to reflect..."
  - "You failed to..." → USE: "Including X would make this more comprehensive..."
  - "This is incomplete..." → USE: "This section could benefit from..."
  - "Critical error..." → USE: "Critical issue requiring immediate attention..."

  CONSTRUCTIVE PATTERNS TO USE:
  - "An opportunity to strengthen this analysis would be..."
  - "Adding X would make this more comprehensive..."
  - "Consider including..."
  - "This section could benefit from..."
  - "A helpful addition would be..."
  - "Building on the strong foundation here, we could enhance..."
```

### Weighted Scoring Display

The template must clearly display the weighted scoring calculation:

```yaml
quality_scores:
  investigation_completeness:
    score: '{score}%'
    weight: 25
    weighted_contribution: '{score * 0.25}'
  technical_accuracy:
    score: '{score}%'
    weight: 20
    weighted_contribution: '{score * 0.20}'
  disposition_reasoning:
    score: '{score}%'
    weight: 20
    weighted_contribution: '{score * 0.20}'
  contextualization:
    score: '{score}%'
    weight: 15
    weighted_contribution: '{score * 0.15}'
  methodology:
    score: '{score}%'
    weight: 10
    weighted_contribution: '{score * 0.10}'
  documentation_quality:
    score: '{score}%'
    weight: 5
    weighted_contribution: '{score * 0.05}'
  cognitive_bias:
    score: '{score}%'
    weight: 5
    weighted_contribution: '{score * 0.05}'
  overall_score: '{calculated_score}%'
  quality_classification: '{Excellent|Good|Needs Improvement|Inadequate}'
```

**Example Output:**
```
Quality Scores:
- Investigation Completeness: 95% (Weight: 25%) → 23.75 points
- Technical Accuracy: 86% (Weight: 20%) → 17.20 points
- Disposition Reasoning: 100% (Weight: 20%) → 20.00 points
- Contextualization: 71% (Weight: 15%) → 10.65 points
- Investigation Methodology: 83% (Weight: 10%) → 8.30 points
- Documentation Quality: 83% (Weight: 5%) → 4.15 points
- Cognitive Bias: 100% (Weight: 5%) → 5.00 points

Overall Quality Score: 89.05% (Good)
```

### JIRA Comment Formatting

Template output must render cleanly in JIRA comments:

- Use markdown headers (`##` for sections, `###` for subsections)
- Use bulleted lists (`-` for items)
- Use bold for emphasis (`**text**`)
- Use code blocks for technical content (` ```text``` `)
- Avoid special characters that break JIRA markdown (e.g., `<`, `>` outside code blocks)
- Keep line length reasonable (<120 chars for readability)

### Testing

**Test Scenarios:**
1. Generate review report for excellent investigation (90%+ score)
2. Generate review report for investigation needing improvement (60-74% score)
3. Generate review report with disposition agreement
4. Generate review report with disposition disagreement
5. Validate blameless language patterns in generated output
6. Validate JIRA comment formatting (post to test ticket)
7. Validate weighted scoring calculation accuracy

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2025-11-09 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

_To be populated during development_

## QA Results

_To be populated during QA_
