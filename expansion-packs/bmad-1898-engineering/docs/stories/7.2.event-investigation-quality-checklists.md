# Story 7.2: Event Investigation Quality Checklists

## Status

Pending

## Story

**As a** Security Reviewer agent,
**I want** specialized quality checklists for reviewing security event investigations,
**so that** I can systematically assess investigation completeness, accuracy, and methodology across 7 quality dimensions.

## Acceptance Criteria

1. Seven event-specific quality checklists created covering all review dimensions
2. Each checklist includes clear pass/fail criteria with scoring methodology
3. Checklists cover Investigation Completeness (25%), Technical Accuracy (20%), Disposition Reasoning (20%), Contextualization (15%), Investigation Methodology (10%), Documentation Quality (5%), Cognitive Bias Detection (5%)
4. Weighted scoring system calculates overall quality score from dimension scores
5. Checklists detect event investigation-specific issues (missing evidence, weak disposition reasoning, incomplete correlation)
6. Checklists compatible with existing execute-checklist.md task
7. Automation bias detection added to cognitive bias checklist

## Tasks / Subtasks

- [ ] Create investigation-completeness-checklist.md (AC: 1, 2, 3, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-completeness-checklist.md`
  - [ ] Define 19 check items covering:
    - [ ] Alert metadata (source, rule ID, severity, timestamps)
    - [ ] Network/host identifiers (source/dest IPs, hostnames, protocols, ports, asset types, criticality)
    - [ ] Investigation steps (logs collected, correlation performed, historical context, asset ownership, business context)
    - [ ] Evidence and analysis (evidence documented, alternative explanations, dead ends, confidence level)
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 25% (highest weight - completeness is critical)
  - [ ] Add examples of complete vs. incomplete investigations
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create investigation-technical-accuracy-checklist.md (AC: 1, 2, 3, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-technical-accuracy-checklist.md`
  - [ ] Define check items covering:
    - [ ] IP addresses and network identifiers correct
    - [ ] Protocol and port information accurate
    - [ ] Alert signature/rule correctly identified
    - [ ] Technical terminology used correctly
    - [ ] Log excerpts interpreted correctly
    - [ ] Attack vectors described accurately
    - [ ] No contradictions between evidence and conclusions
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 20%
  - [ ] Add guidance for verifying technical claims
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create disposition-reasoning-checklist.md (AC: 1, 2, 3, 5, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/disposition-reasoning-checklist.md`
  - [ ] Define check items covering:
    - [ ] Clear disposition stated (TP, FP, BTP)
    - [ ] Reasoning supported by evidence
    - [ ] Alternative explanations considered
    - [ ] Confidence level stated (High/Medium/Low)
    - [ ] Escalation decision justified
    - [ ] Business/operational context factored into decision
    - [ ] Next actions clearly specified
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 20% (disposition accuracy is critical)
  - [ ] Add examples of strong vs. weak reasoning
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create investigation-contextualization-checklist.md (AC: 1, 2, 3, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-contextualization-checklist.md`
  - [ ] Define check items covering:
    - [ ] Asset criticality assessed
    - [ ] Business impact evaluated
    - [ ] Affected systems/services identified
    - [ ] Risk level determined based on context
    - [ ] Client/customer impact considered
    - [ ] SLA/compliance implications noted
    - [ ] Environmental factors explained (test vs. prod, maintenance windows)
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 15%
  - [ ] Add guidance for context assessment
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create investigation-methodology-checklist.md (AC: 1, 2, 3, 5, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-methodology-checklist.md`
  - [ ] Define check items covering:
    - [ ] Hypothesis-driven approach evident
    - [ ] Multiple data sources consulted
    - [ ] Scope appropriately bounded
    - [ ] Investigation steps documented
    - [ ] Dead ends or negative findings noted
    - [ ] Peer consultation or escalation used when appropriate
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 10%
  - [ ] Add examples of rigorous vs. superficial methodology
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create investigation-documentation-quality-checklist.md (AC: 1, 2, 3, 6)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-documentation-quality-checklist.md`
  - [ ] Define check items covering:
    - [ ] Logical structure and flow
    - [ ] Professional tone and language
    - [ ] Minimal typos/grammatical errors
    - [ ] Key findings highlighted or summarized
    - [ ] Evidence references clear and verifiable
    - [ ] Timestamps in consistent format
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 5% (lowest weight - style over substance)
  - [ ] Add style guide references
  - [ ] Compatible with execute-checklist.md task format

- [ ] Create investigation-cognitive-bias-checklist.md (AC: 1, 2, 3, 5, 7)
  - [ ] Create `expansion-packs/bmad-1898-engineering/checklists/investigation-cognitive-bias-checklist.md`
  - [ ] Define check items covering:
    - [ ] No confirmation bias (seeking only supporting evidence)
    - [ ] No anchoring bias (over-reliance on initial alert severity)
    - [ ] No availability bias (over-weighting recent/memorable incidents)
    - [ ] No recency bias (ignoring historical patterns)
    - [ ] **No automation bias (blindly trusting alert system)** ← NEW for event investigations
    - [ ] Alternative hypotheses considered
  - [ ] Define scoring: (Passed Items / Total Items) × 100
  - [ ] Weight: 5%
  - [ ] Add bias detection guidance with examples
  - [ ] Add automation bias definition and detection criteria
  - [ ] Compatible with execute-checklist.md task format

- [ ] Implement weighted scoring system (AC: 4)
  - [ ] Document scoring formula in each checklist header
  - [ ] Calculate dimension scores: (Passed / Total) × 100
  - [ ] Calculate overall quality score: Σ(Dimension Score × Weight)
  - [ ] Example: (Completeness 90% × 0.25) + (Accuracy 85% × 0.20) + ... = Overall Score 88%
  - [ ] Define quality classifications:
    - [ ] Excellent: 90-100%
    - [ ] Good: 75-89%
    - [ ] Needs Improvement: 60-74%
    - [ ] Inadequate: <60%

- [ ] Add event-specific issue detection patterns (AC: 5)
  - [ ] Missing evidence patterns: No logs collected, no correlation performed, no historical context
  - [ ] Weak disposition reasoning: Conclusion without evidence, no alternatives considered, missing confidence level
  - [ ] Incomplete correlation: Single event analyzed in isolation, no timeline reconstruction
  - [ ] Insufficient contextualization: No asset criticality, no business impact assessment
  - [ ] Shallow methodology: No hypothesis stated, single data source only, no negative findings documented
  - [ ] Automation bias indicators: Alert disposition = investigation disposition without independent verification

- [ ] Validate checklist compatibility with existing review workflow (AC: 6)
  - [ ] Test checklists with execute-checklist.md task
  - [ ] Verify markdown format compatibility
  - [ ] Confirm scoring calculation works correctly
  - [ ] Ensure checklist metadata is parseable
  - [ ] Validate weighted scoring aggregation

## Dev Notes

### Epic Context

**Epic 7: Security Event Investigation Review Capability**

This story creates the specialized quality checklists that the Security Reviewer agent will use when reviewing event investigations (Story 7.4). These checklists parallel the existing 8 CVE enrichment checklists but are tailored to the unique aspects of event investigation assessment.

**Related Stories:**
- Story 7.1: Security Analyst Event Investigation Capability (creates investigation documents to review)
- **Story 7.2: Event Investigation Quality Checklists** (This story)
- Story 7.3: Event Investigation Review Report Template (uses checklist results)
- Story 7.4: Security Reviewer Auto-Detection and Event Review (executes these checklists)

**Integration Point:** The Security Reviewer agent (Story 2.1) will execute these checklists via the existing `execute-checklist.md` task when reviewing event investigations.

### Relevant Source Tree

```
expansion-packs/bmad-1898-engineering/
├── checklists/
│   ├── # Existing CVE enrichment checklists (8 total)
│   ├── technical-accuracy-checklist.md            (Existing - CVE)
│   ├── completeness-checklist.md                  (Existing - CVE)
│   ├── actionability-checklist.md                 (Existing - CVE)
│   ├── contextualization-checklist.md             (Existing - CVE)
│   ├── documentation-quality-checklist.md         (Existing - CVE)
│   ├── attack-mapping-validation-checklist.md     (Existing - CVE)
│   ├── cognitive-bias-checklist.md                (Existing - CVE)
│   ├── source-citation-checklist.md               (Existing - CVE)
│   ├── # NEW: Event investigation checklists (7 total)
│   ├── investigation-completeness-checklist.md    (TO BE CREATED)
│   ├── investigation-technical-accuracy-checklist.md (TO BE CREATED)
│   ├── disposition-reasoning-checklist.md         (TO BE CREATED)
│   ├── investigation-contextualization-checklist.md (TO BE CREATED)
│   ├── investigation-methodology-checklist.md     (TO BE CREATED)
│   ├── investigation-documentation-quality-checklist.md (TO BE CREATED)
│   └── investigation-cognitive-bias-checklist.md  (TO BE CREATED)
├── tasks/
│   ├── execute-checklist.md                       (Existing - reuse for event checklists)
│   └── review-security-enrichment.md              (Existing - will extend for events in Story 7.5)
├── agents/
│   └── security-reviewer.md                       (Existing - will extend in Story 7.4)
└── docs/
    ├── prd/
    │   └── epic-7-security-event-investigation-review.md (Epic PRD - checklist specs in FR-2)
    └── stories/
        └── 7.2.event-investigation-quality-checklists.md (This file)
```

**Files to Create:** 7 new checklists in `expansion-packs/bmad-1898-engineering/checklists/`

### Checklist Design Principles

**From Epic 7 PRD (FR-2):**

The 7 event investigation quality dimensions replace the 8 CVE enrichment dimensions because event investigations have different focus areas:

**Dimension Changes:**
- **Added:** Disposition Reasoning (20% weight) - critical for event investigations
- **Added:** Investigation Methodology (10% weight) - assesses investigative rigor
- **Removed:** Actionability (not applicable - events don't have remediation guidance)
- **Removed:** Attack Mapping Validation (not applicable - events don't map to CVE attack chains)
- **Removed:** Source Citation (events rely on internal logs, not external sources)
- **Modified:** Completeness → Investigation Completeness (different criteria)
- **Modified:** Contextualization → Investigation Contextualization (focuses on asset/business context)
- **Modified:** Cognitive Bias → Investigation Cognitive Bias (adds automation bias)

**Weight Distribution Rationale:**
- Investigation Completeness (25%): Highest weight - incomplete investigations are the most common failure mode
- Technical Accuracy (20%): Second highest - incorrect IP/protocol analysis leads to wrong dispositions
- Disposition Reasoning (20%): Critical - the disposition decision is the primary output of investigations
- Contextualization (15%): Important - context determines severity and escalation decisions
- Investigation Methodology (10%): Moderate - good methodology improves consistency but doesn't guarantee correct results
- Documentation Quality (5%): Low - clarity matters but substance over style
- Cognitive Bias Detection (5%): Low - bias detection is valuable but subjective

### Investigation Completeness Checklist (Weight: 25%)

**Purpose:** Verify all required investigation steps were performed and documented.

**19 Check Items (from Epic 7 PRD Appendix B):**

Alert Metadata (5 items):
1. Alert source documented (platform/sensor)
2. Alert rule ID captured
3. Severity level stated
4. Detection timestamp recorded
5. Event occurrence time noted

Network/Host Identifiers (5 items):
6. Source IP/hostname documented
7. Destination IP/hostname documented
8. Protocol and port information
9. Asset type identified
10. Asset criticality assessed

Investigation Steps (5 items):
11. Relevant logs collected
12. Correlation performed (related events)
13. Historical context researched
14. Asset ownership confirmed
15. Business context obtained

Evidence & Analysis (4 items):
16. Evidence documented (log excerpts)
17. Alternative explanations considered
18. Dead ends noted
19. Confidence level stated

**Scoring:** (Passed / 19) × 100 = Dimension Score

### Technical Accuracy Checklist (Weight: 20%)

**Purpose:** Verify factual correctness and technical validity.

**Check Items:**
1. IP addresses and network identifiers correct (no typos, valid ranges)
2. Protocol and port information accurate (TCP/UDP, valid port ranges)
3. Alert signature/rule correctly identified (matches platform documentation)
4. Technical terminology used correctly (no misuse of security terms)
5. Log excerpts interpreted correctly (proper log parsing)
6. Attack vectors described accurately (realistic attack scenarios)
7. No contradictions between evidence and conclusions (logical consistency)

**Scoring:** (Passed / Total) × 100 = Dimension Score

### Disposition Reasoning Checklist (Weight: 20%)

**Purpose:** Verify logical, evidence-based true/false positive determination.

**Check Items:**
1. Clear disposition stated (TP, FP, or BTP explicitly stated)
2. Reasoning supported by evidence (specific evidence cited for disposition)
3. Alternative explanations considered (at least 2 alternative scenarios evaluated)
4. Confidence level stated (High/Medium/Low explicitly assigned)
5. Escalation decision justified (clear logic for escalate/close decision)
6. Business/operational context factored into decision (context influenced disposition)
7. Next actions clearly specified (concrete next steps defined)

**Scoring:** (Passed / 7) × 100 = Dimension Score

**Examples:**

_Strong Disposition Reasoning:_
```
Disposition: False Positive (Confidence: High)

Evidence:
- Source IP 10.50.1.100 is the authorized jump server (Asset DB confirms)
- SSH connection to 10.10.5.25 occurs daily at 02:00 UTC for backup operations (6 months of historical logs)
- Destination 10.10.5.25 is a file server in the same zone (network diagram confirms)
- Connection used authorized SSH keys (key fingerprint matches IT Ops team)

Alternative Explanations Considered:
1. Unauthorized access via stolen jump server credentials → Ruled out: SSH key fingerprint matches authorized key, no concurrent suspicious activity
2. Lateral movement after jump server compromise → Ruled out: Jump server shows no other IOCs, connection pattern matches known backup schedule

Escalation Decision: No escalation required
Next Actions: Tune alert to exclude jump server 10.50.1.100 → file server 10.10.5.25 SSH connections during maintenance windows (00:00-04:00 UTC)
```

_Weak Disposition Reasoning:_
```
Disposition: False Positive

The alert looks like normal SSH traffic. Closing as false positive.

Next Actions: Close ticket
```

### Investigation Contextualization Checklist (Weight: 15%)

**Purpose:** Verify business and operational context integration.

**Check Items:**
1. Asset criticality assessed (Critical/High/Medium/Low assigned)
2. Business impact evaluated (potential consequences described)
3. Affected systems/services identified (downstream dependencies noted)
4. Risk level determined based on context (severity + context = risk)
5. Client/customer impact considered (external impact assessed)
6. SLA/compliance implications noted (regulatory or contractual requirements)
7. Environmental factors explained (test vs. prod, maintenance windows, scheduled changes)

**Scoring:** (Passed / 7) × 100 = Dimension Score

### Investigation Methodology Checklist (Weight: 10%)

**Purpose:** Verify sound investigative process and rigor.

**Check Items:**
1. Hypothesis-driven approach evident (initial hypothesis stated)
2. Multiple data sources consulted (at least 3 log sources or data sources)
3. Scope appropriately bounded (investigation scope defined, not overly broad or narrow)
4. Investigation steps documented (clear sequence of actions taken)
5. Dead ends or negative findings noted (documented what was checked but found nothing)
6. Peer consultation or escalation used when appropriate (sought help when needed)

**Scoring:** (Passed / 6) × 100 = Dimension Score

**Example:**
```
Initial Hypothesis: SSH connection represents unauthorized lateral movement

Investigation Steps:
1. Checked source IP ownership in Asset DB → Authorized jump server
2. Reviewed destination asset in network diagram → File server in same zone
3. Checked SSH key fingerprints in IT Ops key registry → Match found
4. Reviewed 6 months of historical logs → Daily pattern at 02:00 UTC
5. Checked for concurrent suspicious activity on jump server → None found
6. Consulted with IT Ops team → Confirmed scheduled backup operation

Dead Ends:
- Checked source IP against threat intel feeds → No matches (not malicious)
- Reviewed jump server for other IOCs → Clean (no compromise indicators)

Conclusion: False Positive (Authorized backup operation)
```

### Documentation Quality Checklist (Weight: 5%)

**Purpose:** Verify clear, professional, structured documentation.

**Check Items:**
1. Logical structure and flow (investigation follows clear narrative)
2. Professional tone and language (no slang, appropriate terminology)
3. Minimal typos/grammatical errors (<5 errors in document)
4. Key findings highlighted or summarized (executive summary or highlights section)
5. Evidence references clear and verifiable (log excerpts, timestamps, source citations)
6. Timestamps in consistent format (all timestamps use same format, preferably UTC)

**Scoring:** (Passed / 6) × 100 = Dimension Score

### Cognitive Bias Detection Checklist (Weight: 5%)

**Purpose:** Verify objective analysis free from common biases.

**Check Items:**
1. No confirmation bias (investigation considered disconfirming evidence, not just confirming)
2. No anchoring bias (disposition not locked to initial alert severity without independent assessment)
3. No availability bias (didn't over-weight recent similar incidents without justification)
4. No recency bias (considered historical patterns, not just recent events)
5. **No automation bias (didn't blindly accept alert disposition without verification)** ← NEW
6. Alternative hypotheses considered (at least 2 alternative scenarios evaluated)

**Scoring:** (Passed / 6) × 100 = Dimension Score

**Automation Bias Detection Criteria (NEW for Event Investigations):**

Automation bias occurs when analysts over-rely on automated systems (alert platforms, SIEM correlation rules) without independent verification.

**Indicators of Automation Bias:**
- Alert says "High Severity" → Analyst disposition: High Severity (no independent assessment)
- Alert says "Malicious Activity" → Analyst disposition: True Positive (no evidence validation)
- SIEM correlation rule fires → Analyst accepts correlation without reviewing individual events
- No evidence of independent verification beyond alert metadata
- Disposition reasoning: "Alert flagged this as malicious, so marking True Positive"

**Example of Automation Bias:**
```
Alert: Claroty High Severity - Suspicious SSH Connection (Severity: Critical)
Analyst Disposition: True Positive (Critical Severity)
Reasoning: "Alert classified this as Critical and suspicious, confirming as True Positive."

⚠️ AUTOMATION BIAS DETECTED: Analyst accepted alert severity and disposition without independent evidence collection or verification.
```

**Example of Bias-Free Analysis:**
```
Alert: Claroty High Severity - Suspicious SSH Connection (Severity: Critical)
Analyst Disposition: False Positive (Confidence: High)
Reasoning: "Despite alert severity being Critical, investigation reveals:
- Source is authorized jump server (Asset DB verified)
- Connection pattern matches scheduled backup (6 months history)
- SSH keys match IT Ops authorized keys
- No concurrent suspicious activity

While alert correctly detected SSH in control environment, the activity is authorized and expected. Downgrading disposition to False Positive with recommendation to tune alert to exclude authorized backup traffic."

✓ NO AUTOMATION BIAS: Analyst performed independent verification and reached different conclusion than alert system.
```

### Weighted Scoring System

**Formula:**
```
Overall Score = (Completeness × 0.25) + (Accuracy × 0.20) + (Disposition × 0.20) +
                (Context × 0.15) + (Methodology × 0.10) + (Documentation × 0.05) + (Bias × 0.05)
```

**Example Calculation:**
```
Investigation Completeness: 18/19 passed = 95%
Technical Accuracy: 6/7 passed = 86%
Disposition Reasoning: 7/7 passed = 100%
Contextualization: 5/7 passed = 71%
Investigation Methodology: 5/6 passed = 83%
Documentation Quality: 5/6 passed = 83%
Cognitive Bias: 6/6 passed = 100%

Overall Score = (95 × 0.25) + (86 × 0.20) + (100 × 0.20) + (71 × 0.15) + (83 × 0.10) + (83 × 0.05) + (100 × 0.05)
             = 23.75 + 17.2 + 20.0 + 10.65 + 8.3 + 4.15 + 5.0
             = 89.05%
             = "Good" (75-89% range)
```

**Quality Classifications:**
- **Excellent:** 90-100% - Exemplary investigation with minor or no gaps
- **Good:** 75-89% - Solid investigation with some improvements needed
- **Needs Improvement:** 60-74% - Significant gaps requiring revision
- **Inadequate:** <60% - Major deficiencies, investigation must be redone

### Checklist Format Compatibility

**Required Format (for execute-checklist.md compatibility):**

```markdown
# {Checklist Name}

**Weight:** {percentage}%
**Purpose:** {One sentence describing what this checklist assesses}

## Check Items

- [ ] **Item 1 Name** - {Description of what to verify}
- [ ] **Item 2 Name** - {Description of what to verify}
...

**Scoring:**
- Total Items: {count}
- Passed Items: [count]
- Score: (Passed / Total) × 100 = ____%

**Guidance:**
{Additional context, examples, or detection criteria}
```

### Testing

**Test Approach:**
1. Create mock event investigation documents (complete, incomplete, biased)
2. Execute checklists against mock investigations using execute-checklist.md task
3. Verify checklist items fire correctly for gap detection
4. Validate weighted scoring calculation
5. Confirm quality classification thresholds

**Test Scenarios:**
- Excellent investigation (90%+): All criteria met, minor documentation issues only
- Good investigation (75-89%): Complete but missing some context or methodology details
- Needs Improvement (60-74%): Missing evidence, weak disposition reasoning
- Inadequate (<60%): Major gaps in completeness, accuracy errors, no disposition reasoning

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2025-11-09 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

_To be populated during development_

## QA Results

_To be populated during QA_
