# Story 5.5: Security Analysis Review Workflow Deep Dive

## Status

Draft

## Story

**As a** security reviewer or quality assurance lead,
**I want** detailed technical documentation of the review workflow,
**so that** I understand each review stage deeply, can conduct systematic quality evaluations, and provide high-quality constructive feedback.

## Acceptance Criteria

1. Documentation covers all 7 review workflow stages with detailed technical specifications
2. Each stage includes inputs, outputs, actions, success criteria, and timing
3. 8 quality dimension checklists fully documented with scoring methodology
4. Gap categorization rules (Critical/Significant/Minor) with examples
5. Cognitive bias detection methodology with debiasing strategies
6. Fact verification process using authoritative sources
7. Review report generation and feedback delivery best practices

## Tasks / Subtasks

- [ ] Create review workflow deep dive documentation (AC: 1, 2)
  - [ ] Create `docs/user-guide/review-workflow-deep-dive.md`
  - [ ] Document workflow overview and architecture
  - [ ] Detail all 7 stages with technical specifications
- [ ] Document Stage 1: Review Preparation (AC: 2)
  - [ ] Inputs: JIRA ticket ID, enrichment comment or local file
  - [ ] Actions: Extract enrichment, parse into sections, extract claims list
  - [ ] Outputs: Structured enrichment data, claims list, analyst name
  - [ ] Success criteria: Enrichment parsed successfully
  - [ ] Timing: 2-3 minutes
- [ ] Document Stage 2: Systematic Quality Evaluation (AC: 2, 3)
  - [ ] Execute all 8 quality dimension checklists sequentially
  - [ ] Calculate dimension scores (percentage per checklist)
  - [ ] Apply weights: Technical(25%) + Completeness(20%) + Actionability(15%) + Context(15%) + Docs(10%) + ATT&CK(5%) + Bias(5%) + Citations(5%)
  - [ ] Calculate overall quality score (0-100)
  - [ ] Classify: Excellent(90-100), Good(75-89), Needs Improvement(60-74), Inadequate(<60)
  - [ ] Timing: 5-7 minutes
- [ ] Document Stage 3: Gap Identification & Categorization (AC: 2, 4)
  - [ ] Identify gaps from checklist failures
  - [ ] Categorize as Critical/Significant/Minor
  - [ ] Critical: Factual errors, missing priority, incorrect metrics, dangerous advice
  - [ ] Significant: Missing business context, incomplete remediation, ATT&CK errors
  - [ ] Minor: Formatting, typos, optional enhancements
  - [ ] Generate recommendations per gap
  - [ ] Timing: 3-4 minutes
- [ ] Document Stage 4: Cognitive Bias Detection (AC: 2, 5)
  - [ ] Detect 5 bias types: Confirmation, Anchoring, Availability, Overconfidence, Recency
  - [ ] Use cognitive-bias-checklist.md patterns
  - [ ] Generate debiasing recommendations
  - [ ] Score bias dimension (0-100%)
  - [ ] Timing: 2-3 minutes
- [ ] Document Stage 5: Fact Verification (Optional) (AC: 2, 6)
  - [ ] Extract factual claims requiring verification
  - [ ] Verify against authoritative sources: NIST NVD, CISA KEV, FIRST EPSS, vendor advisories
  - [ ] Use Perplexity for additional verification if needed
  - [ ] Document discrepancies with evidence
  - [ ] Timing: 3-5 minutes (if performed)
- [ ] Document Stage 6: Review Report Documentation (AC: 2, 7)
  - [ ] Generate 12-section review report using security-review-report-tmpl.yaml
  - [ ] Apply blameless tone throughout
  - [ ] Strengths-first approach
  - [ ] Constructive language for gaps
  - [ ] Educational resources linked
  - [ ] Conversation starters included
  - [ ] Timing: 2-3 minutes
- [ ] Document Stage 7: Feedback & Improvement Loop (AC: 2, 7)
  - [ ] Post review report as JIRA comment
  - [ ] If critical issues: Assign back to analyst, transition to "In Progress"
  - [ ] If approved: Transition to "Remediation Planning"
  - [ ] Save review report locally: `reviews/{ticket-id}-review.md`
  - [ ] Log metrics: `metrics/review-metrics.csv`
  - [ ] Timing: 1 minute
- [ ] Document 8 quality dimension checklists (AC: 3)
  - [ ] Technical Accuracy checklist (25 weight)
  - [ ] Completeness checklist (20 weight)
  - [ ] Actionability checklist (15 weight)
  - [ ] Contextualization checklist (15 weight)
  - [ ] Documentation Quality checklist (10 weight)
  - [ ] Attack Mapping Validation checklist (5 weight)
  - [ ] Cognitive Bias checklist (5 weight)
  - [ ] Source Citation checklist (5 weight)
- [ ] Create gap categorization decision tree (AC: 4)
  - [ ] Critical gap criteria and examples
  - [ ] Significant gap criteria and examples
  - [ ] Minor gap criteria and examples
  - [ ] Edge case handling
- [ ] Document cognitive bias patterns (AC: 5)
  - [ ] Confirmation bias: Seeking only confirming evidence
  - [ ] Anchoring bias: Over-relying on first information
  - [ ] Availability bias: Overweighting memorable incidents
  - [ ] Overconfidence bias: Unwarranted certainty
  - [ ] Recency bias: Focusing on recent data only
  - [ ] Detection patterns and indicators
  - [ ] Debiasing strategies
- [ ] Create fact verification guide (AC: 6)
  - [ ] Authoritative source hierarchy: NIST NVD > CISA > Vendor > Industry
  - [ ] Verification procedures per claim type
  - [ ] Discrepancy documentation format
  - [ ] When to perform fact verification (P1/P2 mandatory, P3-P5 sampling)

## Dev Notes

### Epic Context

**Parent Epic:** Epic 5: User Documentation & Usage Guide
**This Story (5.5) Purpose:** Provide deep technical reference for review workflow, enabling reviewers to conduct systematic, high-quality peer reviews with blameless constructive feedback.

**Dependencies:**
- Story 5.1: Installation & Initial Setup Guide
- Story 5.3: Security Reviewer Agent Usage Guide (user-level review documentation)
- Story 3.2: Security Analysis Review Workflow (implementation reference)

**Related Stories:**
- Story 5.4: Security Alert Enrichment Workflow Deep Dive (understanding what's being reviewed)
- Story 5.10: Troubleshooting, FAQ & Best Practices

### Source Workflow Definition

**Workflow File:** `expansion-packs/bmad-1898-engineering/workflows/security-analysis-review-workflow.yaml`
**Orchestration Task:** `expansion-packs/bmad-1898-engineering/tasks/review-security-enrichment.md`

### 7-Stage Review Workflow Technical Specifications

**Overall Workflow:**
- **ID:** security-analysis-review-v1
- **Estimated Duration:** 15-20 minutes
- **Prerequisites:** Enriched JIRA ticket, Security Reviewer agent, Atlassian MCP, (Optional) Perplexity MCP
- **Success Metrics:** <20 min total, all stages complete, review posted, constructive feedback

---

**Stage 1: Review Preparation**
- **Duration:** 2-3 minutes
- **Inputs:**
  - JIRA ticket ID (enriched ticket)
  - Source: JIRA comment or local file `enrichments/{ticket-id}-enrichment.md`
- **Actions:**
  1. Retrieve enrichment from JIRA using `mcp__atlassian__getJiraIssue`
  2. Extract enrichment comment (last comment by Security Analyst or search by pattern)
  3. If not in JIRA, load from local file `enrichments/{ticket-id}-enrichment.md`
  4. Parse enrichment into 12 sections
  5. Extract factual claims requiring verification (CVSS, EPSS, KEV, patch versions, exploit status)
  6. Extract analyst name from JIRA or enrichment metadata
  7. Create structured enrichment object for review stages
- **Outputs:**
  - Structured enrichment data (12 sections parsed)
  - Claims list with claim type and source citation
  - Analyst name
  - Enrichment timestamp
  - CVE ID and ticket metadata
- **Success Criteria:**
  - Enrichment successfully retrieved and parsed
  - All 12 sections identified (even if some empty)
  - Claims list extracted (minimum: CVSS, EPSS, KEV, Priority)
- **Error Handling:**
  - **Enrichment Not Found:** Check JIRA comments, check local file, prompt user for enrichment location
  - **Parsing Failure:** Malformed markdown → Manual section identification, flag formatting issue
  - **Missing Sections:** Document as completeness gap (will be caught in Stage 2)
- **Performance Optimization:**
  - Cache JIRA ticket data (avoid redundant MCP calls if multiple reviews)
  - Parallel parsing of sections
  - Pre-compiled regex patterns for section headers
- **Common Issues:**
  - Multiple enrichment comments in JIRA → Use most recent or prompt for selection
  - Enrichment in non-standard format → Attempt flexible parsing, flag for analyst

---

**Stage 2: Systematic Quality Evaluation**
- **Duration:** 5-7 minutes
- **Inputs:**
  - Structured enrichment data (from Stage 1)
- **Actions:**
  1. Execute 8 quality dimension checklists sequentially:

     **Checklist 1: Technical Accuracy (25% weight)**
     - Verify CVSS score matches NVD or vendor advisory
     - Verify CVSS vector string is valid (syntax: AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)
     - Verify EPSS score is current (<7 days old) and in valid range (0-100%)
     - Verify KEV status against CISA KEV catalog
     - Verify affected versions match vendor advisory
     - Verify patched versions are specific and correct
     - Verify exploit status matches threat intelligence
     - Verify ATT&CK T-numbers are valid (exist in MITRE matrix)
     - **Scoring:** Pass/Fail per item → percentage (e.g., 7/8 = 87.5%)

     **Checklist 2: Completeness (20% weight)**
     - Verify all 12 template sections populated
     - Executive Summary present and concise (2-3 sentences)
     - Vulnerability Details complete
     - Severity Metrics included
     - Affected Systems documented
     - Remediation Guidance provided
     - MITRE ATT&CK Mapping present
     - Business Impact Assessment included
     - Threat Intelligence researched
     - Verification steps provided
     - References cited
     - Related CVEs researched
     - Analyst Notes included
     - **Scoring:** Sections complete / 12 → percentage

     **Checklist 3: Actionability (15% weight)**
     - Remediation steps are clear and specific (not vague)
     - Patch version or workaround provided
     - Verification steps included
     - Compensating controls listed (if applicable)
     - Guidance appropriate for target audience
     - Estimated effort noted
     - **Scoring:** Actionable items present / 6 → percentage

     **Checklist 4: Contextualization (15% weight)**
     - Asset Criticality Rating assessed
     - System Exposure classified
     - Business processes affected identified
     - Business impact clearly articulated
     - Priority rationale references business context
     - Stakeholders identified
     - **Scoring:** Context elements present / 6 → percentage

     **Checklist 5: Documentation Quality (10% weight)**
     - Markdown formatting correct
     - Spelling and grammar professional
     - Section headings consistent
     - Lists and tables formatted properly
     - Clarity (no ambiguous language)
     - Organization logical
     - **Scoring:** Quality elements present / 6 → percentage

     **Checklist 6: Attack Mapping Validation (5% weight)**
     - Tactics are valid ATT&CK tactics
     - Techniques have valid T-numbers (format: T1234)
     - Mapping appropriate for vulnerability type
     - Detection implications included
     - Defense recommendations aligned
     - **Scoring:** Mapping elements correct / 5 → percentage

     **Checklist 7: Cognitive Bias (5% weight)**
     - (Performed in Stage 4, score incorporated here)
     - Bias-free analysis → 100%
     - Biases detected → lower score based on severity

     **Checklist 8: Source Citation (5% weight)**
     - All factual claims have sources
     - Sources are authoritative (NIST, CISA, vendor)
     - URLs included for all sources
     - Sources are current
     - No reliance on non-authoritative sources alone
     - **Scoring:** Citation elements present / 5 → percentage

  2. Calculate weighted overall quality score:
     ```
     Quality Score = (
       Technical_Accuracy × 0.25 +
       Completeness × 0.20 +
       Actionability × 0.15 +
       Contextualization × 0.15 +
       Documentation_Quality × 0.10 +
       Attack_Mapping × 0.05 +
       Cognitive_Bias × 0.05 +
       Source_Citation × 0.05
     )
     ```

  3. Classify overall quality:
     - 90-100: Excellent
     - 75-89: Good
     - 60-74: Needs Improvement
     - < 60: Inadequate

  4. Generate quality dimension scores table for report

- **Outputs:**
  - Quality dimension scores (8 dimensions, 0-100% each)
  - Weighted overall quality score (0-100)
  - Quality classification (Excellent/Good/Needs Improvement/Inadequate)
  - Checklist results (pass/fail items per dimension)
  - Gaps identified per dimension
- **Success Criteria:**
  - All 8 checklists executed
  - Quality score calculated correctly (validated against manual calculation)
  - Classification assigned
- **Error Handling:**
  - **Missing Data for Verification:** Mark as "Unable to verify" and note in gap
  - **Checklist Execution Error:** Skip problematic item, note in review, continue
- **Performance Optimization:**
  - Parallel checklist execution where possible
  - Cache external verification data (NVD, KEV catalog)
  - Skip optional verifications for low-priority reviews (P4/P5)
- **Common Issues:**
  - CVSS vector syntax variations (v3.0 vs v3.1) → Normalize before validation
  - EPSS score precision differences → Accept ±0.5% variance

---

**Stage 3: Gap Identification & Categorization**
- **Duration:** 3-4 minutes
- **Inputs:**
  - Checklist results (from Stage 2)
  - Failed checklist items (gaps)
- **Actions:**
  1. Extract all failed checklist items (gaps)
  2. Categorize each gap using decision rules:

     **Critical Gaps (Immediate Action Required):**
     - Factual errors: Incorrect CVSS, wrong patch version, KEV status error
     - Missing priority assessment or priority incorrect
     - Incorrect severity metrics (off by >1.0 CVSS points)
     - Dangerous remediation advice (could cause harm)
     - Invalid ATT&CK T-numbers that don't exist
     - Missing or incorrect remediation guidance

     **Significant Gaps (Should Fix):**
     - Missing business context (ACR or Exposure)
     - Incomplete remediation (no workarounds or verification)
     - ATT&CK mapping errors (wrong tactic/technique)
     - Weak or missing source citations
     - Vague remediation guidance (not actionable)
     - Business impact not articulated

     **Minor Gaps (Nice to Have):**
     - Formatting inconsistencies
     - Spelling or grammar errors
     - Missing optional sections (Related CVEs if truly none exist)
     - Minor documentation quality issues
     - Optional enhancements

  3. For each gap, generate:
     - **Description:** What is missing or incorrect
     - **Impact:** Why it matters (risk of gap)
     - **Recommendation:** Specific fix action
     - **Resource Link:** Where to learn how to fix
     - **Estimated Effort:** Time to remediate (minutes)

  4. Prioritize gaps: Critical → Significant → Minor

- **Outputs:**
  - Categorized gaps list (Critical/Significant/Minor)
  - Gap details (description, impact, recommendation, resource, effort)
  - Prioritized recommendations
  - Total estimated remediation effort
- **Success Criteria:**
  - All gaps categorized correctly
  - Each gap has actionable recommendation
  - Resources provided for all gaps
- **Error Handling:**
  - **Ambiguous Gap Categorization:** Default to higher severity (conservative), note uncertainty
  - **No Gaps Found:** Validate this is correct (Excellent enrichment), document strengths
- **Common Issues:**
  - Edge case gaps (CVSS 6.9 vs 7.0 threshold) → Apply strict rules, note in recommendation
  - Multiple gaps of same type → Consolidate into single finding with examples

---

**Stage 4: Cognitive Bias Detection**
- **Duration:** 2-3 minutes
- **Inputs:**
  - Structured enrichment data (from Stage 1)
  - Threat Intelligence section
  - Priority Assessment rationale
- **Actions:**
  1. Detect 5 bias types using cognitive-bias-checklist.md patterns:

     **Bias Type 1: Confirmation Bias**
     - Pattern: Seeking only evidence that confirms initial assessment
     - Indicators:
       - Threat Intelligence section focuses only on exploitation evidence, ignores non-exploitation
       - Remediation research considers only one approach (patch), ignores workarounds
       - Priority rationale cherry-picks factors supporting high priority, ignores mitigating factors
     - Debiasing: "Consider alternative perspectives. Research both exploitation and non-exploitation indicators."

     **Bias Type 2: Anchoring Bias**
     - Pattern: Over-relying on first piece of information (initial CVSS)
     - Indicators:
       - Priority based solely on CVSS, ignoring EPSS/KEV/ACR/Exposure
       - Initial severity estimate dominates final assessment despite contradicting evidence
       - Vendor advisory CVSS accepted without verification against NVD
     - Debiasing: "Verify initial CVSS against multiple sources. Consider all priority factors equally."

     **Bias Type 3: Availability Bias**
     - Pattern: Overweighting recent or memorable incidents
     - Indicators:
       - Business impact exaggerated based on recent similar incident
       - Threat intelligence focuses on recent attack, ignores historical context
       - Priority escalated due to recent publicity (Log4Shell effect)
     - Debiasing: "Balance recent incidents with historical data. Assess this CVE on its own merits."

     **Bias Type 4: Overconfidence Bias**
     - Pattern: Expressing certainty without sufficient evidence
     - Indicators:
       - Definitive statements without sources: "This will be exploited" vs "EPSS suggests high probability"
       - Priority assigned without considering uncertainty
       - Remediation advice presented as only option without acknowledging alternatives
     - Debiasing: "Acknowledge uncertainty. Use probabilistic language. Cite evidence for claims."

     **Bias Type 5: Recency Bias**
     - Pattern: Focusing on most recent data, ignoring historical context
     - Indicators:
       - Threat intelligence considers only last 30 days
       - Related CVEs research limited to recent publications
       - Priority influenced by recent organizational focus
     - Debiasing: "Include historical context. Research CVE exploitation over time, not just recent activity."

  2. For each detected bias:
     - Document bias type
     - Provide specific example from enrichment
     - Generate debiasing recommendation
     - Assess severity (Minor/Moderate/Significant)

  3. Score Cognitive Bias dimension:
     - No biases detected: 100%
     - Minor bias: 80-90%
     - Moderate bias: 60-79%
     - Significant bias: <60%

- **Outputs:**
  - Detected biases list (0-5 biases)
  - Bias details (type, example, debiasing recommendation, severity)
  - Cognitive Bias dimension score (0-100%)
- **Success Criteria:**
  - All 5 bias types checked
  - Examples provided for detected biases
  - Debiasing recommendations are actionable
- **Error Handling:**
  - **Uncertain Bias Detection:** Note as "Possible [bias type]" with caveat, provide debiasing anyway
  - **No Bias Indicators:** Document as "No cognitive biases detected" (positive finding)
- **Common Issues:**
  - Borderline bias cases → Document as learning opportunity, not critical gap
  - Bias in low-priority enrichments (P4/P5) → Note but don't escalate severity

---

**Stage 5: Fact Verification (Optional)**
- **Duration:** 3-5 minutes (if performed)
- **When to Perform:**
  - Mandatory: P1/P2 priorities (critical/high severity)
  - Optional: P3 (25% sampling)
  - Skip: P4/P5 (unless Technical Accuracy score <70%)
- **Inputs:**
  - Claims list (from Stage 1)
  - Factual claims requiring verification: CVSS, EPSS, KEV, affected versions, patched versions, exploit status
- **Actions:**
  1. Verify CVSS score and vector:
     - Source: NIST NVD (https://nvd.nist.gov/vuln/detail/CVE-YYYY-NNNNN)
     - Cross-check: Vendor security advisory
     - Discrepancy threshold: >0.5 CVSS points

  2. Verify EPSS score:
     - Source: FIRST EPSS (https://www.first.org/epss/)
     - Check date: Must be within 7 days
     - Discrepancy threshold: >5% EPSS difference

  3. Verify CISA KEV status:
     - Source: CISA KEV Catalog (https://www.cisa.gov/known-exploited-vulnerabilities-catalog)
     - Search by CVE ID
     - Check date added if KEV=Yes
     - Discrepancy: Yes vs No = Critical gap

  4. Verify affected and patched versions:
     - Source: Vendor security advisory (primary)
     - Cross-check: NVD affected configurations
     - Discrepancy: Version mismatch, missing version ranges

  5. Verify exploit status:
     - Sources: Exploit-DB, Metasploit, GitHub (PoC searches)
     - Optional: Use Perplexity for recent threat intelligence
     - Discrepancy: "None" claimed but PoC exists = Significant gap

  6. Document all discrepancies:
     - Claim made in enrichment
     - Actual value per authoritative source
     - Source URL with timestamp
     - Severity of discrepancy (Critical/Significant/Minor)

- **Outputs:**
  - Fact verification results (verified claims count)
  - Discrepancies found (with evidence and sources)
  - Verification timestamp
  - Sources consulted list
- **Success Criteria:**
  - All mandatory claims verified (CVSS, EPSS, KEV, Priority factors)
  - Discrepancies documented with authoritative sources
  - Verification completed within 5 minutes
- **Error Handling:**
  - **Source Unavailable:** Note "Unable to verify - source unavailable" and date, recommend re-verification
  - **Conflicting Sources:** Document all sources and conflict, recommend analyst clarification
  - **Timeout:** Skip optional verifications, focus on critical claims (CVSS, KEV)
- **Performance Optimization:**
  - Parallel verification of independent claims
  - Cache NVD/KEV/EPSS data for batch reviews
  - Skip verification if enrichment includes proper source citations with URLs
- **Common Issues:**
  - NVD slow or unavailable → Use vendor advisory as primary source
  - EPSS data lag (new CVE <48 hours) → Note as "Preliminary - EPSS pending"
  - Vendor CVSS differs from NVD → Document both, explain difference (temporal vs base)

---

**Stage 6: Review Report Documentation**
- **Duration:** 2-3 minutes
- **Inputs:**
  - Quality dimension scores (from Stage 2)
  - Categorized gaps (from Stage 3)
  - Cognitive bias assessment (from Stage 4)
  - Fact verification results (from Stage 5, if performed)
- **Actions:**
  1. Generate 12-section review report using security-review-report-tmpl.yaml:

     **Section 1: Review Metadata**
     - Ticket ID, CVE ID, Analyst name, Reviewer name, Review date
     - Overall quality score, Classification

     **Section 2: Executive Summary**
     - 2-3 sentences, constructive tone
     - Overall assessment (Excellent/Good/Needs Improvement)
     - Key strengths and gaps summary
     - Example: "Solid enrichment with accurate technical analysis and good remediation guidance. Some gaps in business context detail and source citations could be improved. Overall assessment: Good."

     **Section 3: Strengths & What Went Well**
     - Always lead with strengths (blameless culture)
     - Acknowledge specific positive work
     - Example: "Exceptional technical accuracy - all metrics verified against multiple sources"
     - Aim for 3-5 specific strengths

     **Section 4: Quality Dimension Scores**
     - Table format with 8 dimensions, scores, weighted scores, assessment
     - Overall total at bottom

     **Section 5: Critical Issues** (if any)
     - List all Critical gaps with impact and recommendations
     - Flag for immediate attention
     - Only include if gaps exist

     **Section 6: Significant Gaps**
     - List all Significant gaps with constructive language
     - Recommendations with effort estimates
     - Learning resources linked

     **Section 7: Minor Improvements**
     - Optional enhancements, lower priority
     - Phrased as suggestions, not requirements

     **Section 8: Cognitive Bias Assessment**
     - Detected biases with non-judgmental language
     - Debiasing recommendations
     - Or "No cognitive biases detected" if clean

     **Section 9: Fact Verification Results** (if performed)
     - Verified claims count
     - Discrepancies documented with evidence
     - Or "Fact verification not performed" if skipped

     **Section 10: Recommendations & Learning Resources**
     - Prioritized: Critical → Significant → Minor
     - Each with specific next steps
     - Links to knowledge base, checklists, guides

     **Section 11: Conversation Starters**
     - Educational questions to foster discussion
     - Example: "What challenges did you encounter researching this CVE?"
     - Encourage collaborative learning

     **Section 12: Next Steps**
     - Clear action items for analyst
     - Timeline expectations
     - Re-review requirements if applicable

  2. Apply blameless tone throughout:
     - Avoid blame language ("you failed" → "this section could be improved")
     - Focus on work, not person
     - Assume good intent
     - Growth-oriented framing

  3. Validate report completeness (all sections populated)
  4. Generate markdown document

- **Outputs:**
  - Review report (12 sections, markdown format)
  - Blameless constructive feedback
  - Actionable recommendations
  - Learning resources linked
- **Success Criteria:**
  - All 12 sections populated
  - Blameless tone validated (no blame language detected)
  - Strengths section present (even if gaps found)
  - Recommendations are specific and actionable
- **Error Handling:**
  - **No Strengths Identified:** Look harder - even inadequate enrichments have some positive elements (effort, structure)
  - **Report Too Long:** Summarize, provide detail in linked gap findings
- **Performance Optimization:**
  - Template-based generation (automated)
  - Pre-written language for common gaps
- **Common Issues:**
  - Tone slips into blame language → Automated tone checker, rephrase
  - Too much detail in executive summary → Limit to 3 sentences max

---

**Stage 7: Feedback & Improvement Loop**
- **Duration:** 1 minute
- **Inputs:**
  - Review report (from Stage 6)
  - Quality classification (Excellent/Good/Needs Improvement/Inadequate)
  - Critical issues flag (present or not)
- **Actions:**
  1. Post review report as JIRA comment using `mcp__atlassian__addCommentToJiraIssue`
     - Format: Markdown
     - Content: Full 12-section review report

  2. Transition JIRA ticket based on review outcome:
     - If Critical issues: Transition to "In Progress" (back to analyst)
     - If Approved (no Critical, quality ≥75%): Transition to "Remediation Planning"
     - If Needs Improvement (no Critical, quality 60-74%): Optional - analyst decides to fix or proceed

  3. Assign JIRA ticket:
     - If returning to analyst: Assign to original analyst
     - If approved: Assign to remediation team or leave assigned to analyst

  4. Save review report locally:
     - Location: `reviews/{ticket-id}-review.md`
     - Format: Markdown (identical to JIRA comment)

  5. Log review metrics:
     - Append to: `metrics/review-metrics.csv`
     - Columns: ticket_id, cve_id, analyst, reviewer, review_date, review_duration_min, quality_score, quality_classification, critical_gaps_count, significant_gaps_count, minor_gaps_count, approval_status

  6. Send notifications (optional, if configured):
     - Email analyst with review summary
     - Slack notification if configured
     - Primary: JIRA assignment notification (automatic)

- **Outputs:**
  - JIRA comment posted (review report)
  - JIRA ticket transitioned (status updated)
  - JIRA ticket assigned (to analyst or remediation team)
  - Local review file saved
  - Metrics CSV entry appended
  - Notifications sent (if configured)
- **Success Criteria:**
  - JIRA comment posted successfully
  - Ticket transitioned to correct status
  - Local file saved
  - Metrics logged
- **Error Handling:**
  - **JIRA Comment Post Failure:** Save locally, provide manual copy instructions
  - **Ticket Transition Failure:** Verify workflow permissions, manual transition if needed
  - **Assignment Failure:** Manual assignment, note in review
  - **Metrics Logging Failure:** Log to backup file, investigate CSV permissions
- **Performance Optimization:**
  - Parallel JIRA operations (comment + transition + assign) if possible
  - Async local file write and metrics logging
- **Common Issues:**
  - JIRA workflow doesn't allow transition (workflow rules) → Override with admin permissions or manual
  - Analyst no longer active (assignment fails) → Assign to team lead or project admin

---

### 8 Quality Dimension Checklists

_(Full checklist specifications documented in Stage 2 above, with examples of each dimension)_

### Gap Categorization Decision Tree

**Decision Process:**
1. Identify failed checklist item (gap)
2. Assess impact of gap:
   - Could lead to incorrect remediation? → Critical
   - Could lead to wrong priority or delay? → Critical
   - Is factually incorrect? → Critical or Significant (depending on claim type)
   - Is missing important context? → Significant
   - Is cosmetic or optional? → Minor
3. Categorize using rules (see Stage 3)
4. Generate recommendation based on gap type

**Examples:**

**Critical Gap Examples:**
- "CVSS score is 8.1 but enrichment states 9.3" → Factual error
- "KEV status is Yes per CISA but enrichment states No" → Critical metadata error
- "Remediation states 'reboot server' but vulnerability requires patch installation" → Dangerous advice
- "Priority assessment missing - cannot determine SLA" → Missing critical section

**Significant Gap Examples:**
- "Business impact states 'affects systems' - no specific processes identified" → Vague context
- "Remediation provides patch but no workaround for systems that can't patch" → Incomplete guidance
- "ATT&CK technique T1190 stated but vulnerability is local privilege escalation (should be T1068)" → Wrong mapping
- "EPSS score cited without source URL" → Missing citation

**Minor Gap Examples:**
- "Spelling error: 'critial' should be 'critical'" → Typo
- "Table formatting inconsistent (missing header row)" → Formatting issue
- "Related CVEs section states 'None' - could check for CVEs in same product from same month" → Optional enhancement
- "Analyst Notes section is brief (1 sentence) - could expand with methodology notes" → Nice-to-have detail

### Cognitive Bias Detection Patterns

_(Full bias patterns documented in Stage 4 above, with detection indicators and debiasing strategies for all 5 bias types)_

### Fact Verification Authoritative Source Hierarchy

**Tier 1: Primary Authoritative Sources (Always Trust)**
- NIST NVD (nvd.nist.gov) - CVSS scores, affected configurations
- CISA KEV Catalog (cisa.gov/known-exploited-vulnerabilities-catalog) - KEV status
- FIRST EPSS (first.org/epss) - EPSS scores
- Vendor Security Advisories (vendor official sites) - Product-specific details

**Tier 2: Secondary Authoritative Sources (Cross-Reference)**
- CVE.org (Mitre) - CVE metadata
- Security bulletins (Microsoft, Adobe, Oracle, etc.)
- CWE database (cwe.mitre.org) - Vulnerability types
- MITRE ATT&CK (attack.mitre.org) - Tactics and techniques

**Tier 3: Industry Sources (Context Only)**
- Exploit-DB - Exploit availability
- Metasploit - Exploit modules
- Security blogs (trusted: Krebs, Schneier, vendor blogs)
- Threat intelligence platforms

**Tier 4: Community Sources (Verify Before Using)**
- GitHub PoC repositories - Treat as indicators, verify
- Reddit/forums - Context only, never sole source
- Social media - Breaking news only, verify immediately

**Verification Priority:**
1. CVSS, KEV, Priority - Always verify (Tier 1 only)
2. EPSS, Patched Versions - Verify for P1/P2 (Tier 1 + Tier 2)
3. Exploit Status - Verify if claimed "Active" (Tier 2 + Tier 3)
4. ATT&CK Mapping - Spot check T-numbers (Tier 2)
5. Threat Intelligence - Context verification (Tier 3)

## Testing

**Test Location:** `expansion-packs/bmad-1898-engineering/tests/documentation/`

**Test Cases:**

1. **TC-DOC-050: Review Workflow Stage Documentation Accuracy**
   - Objective: Verify all 7 stages match implementation
   - Test: Compare documentation to workflow YAML and task implementation
   - Expected: 100% alignment

2. **TC-DOC-051: Quality Dimension Checklist Verification**
   - Objective: Verify all 8 checklists documented accurately
   - Test: Execute review using documentation, compare to actual checklist files
   - Expected: All checklist items match, scoring methodology correct

3. **TC-DOC-052: Gap Categorization Accuracy**
   - Objective: Verify gap categorization rules are correct
   - Test: Create enrichments with known gaps, categorize using docs, compare to expert categorization
   - Expected: 90% agreement on gap severity

4. **TC-DOC-053: Cognitive Bias Detection Patterns**
   - Objective: Verify bias detection patterns are accurate
   - Test: Create enrichments with known biases, detect using docs
   - Expected: All planted biases detected

5. **TC-DOC-054: Fact Verification Process**
   - Objective: Verify fact verification procedures are correct
   - Test: Follow verification guide for known CVE, compare to actual values
   - Expected: All discrepancies detected, sources validated

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2025-11-08 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

_(To be populated during implementation)_

## QA Results

_(To be populated during QA review)_
